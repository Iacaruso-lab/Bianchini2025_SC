{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c84c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define relevants paths and load functions and libraries\n",
    "\n",
    "%run Bianchini2025_SC\\\\Analysis\\\\helper_functions\\\\functions_analysis.py\n",
    "    \n",
    "data_path = 'Bianchini2025_SC\\\\Datasets\\\\' # your data path\n",
    "saving_path = 'Bianchini2025_SC\\\\Figures_output\\\\' # your saving figures path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56b00a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [01:34<00:00,  4.72s/it]\n"
     ]
    }
   ],
   "source": [
    "# extract data for confusion matrix figure 4B and 4C\n",
    "# load the outputs from this code run_classifier_increasing_N.py\n",
    "\n",
    "# compare it to randomised labels\n",
    "this_path = ''.join([data_path,'decoder_datasets\\\\clf_increasing_N_SVM\\\\'])\n",
    "n_neurons = 5360\n",
    "n_iter = 20\n",
    "max_n = n_neurons\n",
    "this_n = np.arange(0, max_n, 25)\n",
    "\n",
    "# Ensure max_n is included\n",
    "if this_n[-1] != max_n:\n",
    "    this_n = np.append(this_n, max_n-1)\n",
    "    \n",
    "all_scores_random = np.full((n_iter, n_neurons), np.nan)\n",
    "\n",
    "# load each repetition infividually and store the mean accuracy \n",
    "for i in tqdm(range(n_iter)):\n",
    "    file =''.join([this_path,f'PRED_rep{i}_random_SVM.npy'])     \n",
    "    all_predicted=np.load(file)\n",
    "    file =''.join([this_path,f'TEST_rep{i}_random_SVM.npy'])\n",
    "    Y_test=np.load(file)\n",
    "\n",
    "    pred = all_predicted[i,:,:,:]\n",
    "    test = Y_test[i,:,:,:]\n",
    "\n",
    "\n",
    "    for n in this_n: # for each neuron\n",
    "        delay_score = np.zeros((pred.shape[1],1))\n",
    "        for k in range(pred.shape[1]): # for each kfold\n",
    "            cm = confusion_matrix(test[n,k,:],pred[n,k,:], normalize='true')\n",
    "            # extract the score for delays\n",
    "            score = np.diag(cm)\n",
    "            delay_score[k] = np.mean(score[:11])    \n",
    "        # save the scores for the main figure\n",
    "        all_scores_random[i,n] = np.mean(delay_score)*100\n",
    "        \n",
    "# and it get saved so it is easier for th eplotting ot just load it\n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\decoder_accuracy_all_neurons_SVM_random.npy'])\n",
    "np.save(save_dir,all_scores_random)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10964a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [03:41<00:00, 11.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# extract data for confusion matrix figure 4B and 4C\n",
    "# load the outputs from this code run_classifier_increasing_N.py\n",
    "\n",
    "n_iter = 20\n",
    "n_neurons = 5360\n",
    "\n",
    "# initiate empty lists to append \n",
    "pred_all =[]\n",
    "test_all =[]\n",
    "all_scores = np.full((n_iter, n_neurons), np.nan)\n",
    "\n",
    "# Create an array from 0 to max_n with a step of 50\n",
    "max_n = n_neurons\n",
    "this_n = np.arange(0, max_n, 25)\n",
    "\n",
    "# Ensure max_n is included\n",
    "if this_n[-1] != max_n:\n",
    "    this_n = np.append(this_n, max_n-1)\n",
    "    \n",
    "# load each repetition infividually and store the mean accuracy \n",
    "for i in tqdm(range(all_scores.shape[0])):\n",
    "    file =''.join([this_path,f'PRED_rep{i}_SVM.npy'])     \n",
    "    all_predicted=np.load(file)\n",
    "    file =''.join([this_path,f'TEST_rep{i}_SVM.npy'])\n",
    "    Y_test=np.load(file)\n",
    "\n",
    "    pred = all_predicted[i,:,:,:]\n",
    "    test = Y_test[i,:,:,:]\n",
    "    \n",
    "    for n in this_n: # for each neuron\n",
    "        delay_score = np.zeros((pred.shape[1],1))\n",
    "        for k in range(pred.shape[1]): # for each kfold\n",
    "            cm = confusion_matrix(test[n,k,:],pred[n,k,:], normalize='true')            \n",
    "            # extract the score for delays\n",
    "            score = np.diag(cm)\n",
    "            delay_score[k] = np.mean(score[:11])    \n",
    "       \n",
    "        if n == pred.shape[0]-1: # to plot the confusion matrix for all neurons\n",
    "            pred_all.append(pred[n,:,:].reshape(-1))\n",
    "            test_all.append(test[n,:,:].reshape(-1))             \n",
    "\n",
    "        # save the scores for the main figure\n",
    "        all_scores[i,n] = np.mean(delay_score)*100\n",
    "\n",
    "pred_all = np.array(pred_all)\n",
    "test_all = np.array(test_all)\n",
    "\n",
    "# and it get saved so it is easier for the plotting ot just load it\n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\decoder_accuracy_all_neurons_SVM.npy'])\n",
    "np.save(save_dir,all_scores)       \n",
    "\n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\predicted_accuracy_all_neurons_SVM_CM.npy'])\n",
    "np.save(save_dir,pred_all) \n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\observed_accuracy_all_neurons_SVM_CM.npy'])\n",
    "np.save(save_dir,test_all) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0e97644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [01:27<00:00,  2.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# increased number of repetitions and do it for the first 200 neurons\n",
    "# load the outputs from this code run_classifier_200_N.py\n",
    "\n",
    "file =''.join([data_path,'decoder_datasets\\\\clf_increasing_N_SVM\\\\PRED_first200neurons_SVM.npy'])     \n",
    "all_predicted=np.load(file)\n",
    "file =''.join([data_path,'decoder_datasets\\\\clf_increasing_N_SVM\\\\TEST_first200neurons_SVM.npy'])\n",
    "Y_test=np.load(file)\n",
    "\n",
    "# initiate empty lists to append \n",
    "n_iter = all_predicted.shape[0]\n",
    "n_neurons = all_predicted.shape[1]\n",
    "all_scores_200 = np.zeros((n_iter,n_neurons))   \n",
    "\n",
    "\n",
    "for i in tqdm(range(all_scores_200.shape[0])):\n",
    "    pred = all_predicted[i,:,:,:]\n",
    "    test = Y_test[i,:,:,:]\n",
    "\n",
    "    for n in range(n_neurons): # for each neuron\n",
    "        delay_score = np.zeros((pred.shape[1],1))\n",
    "        for k in range(pred.shape[1]): # for each kfold\n",
    "            cm = confusion_matrix(test[n,k,:],pred[n,k,:],normalize='true')\n",
    "            # extract the score for delays\n",
    "            score = np.diag(cm)\n",
    "            delay_score[k] = np.mean(score[:11])    \n",
    "\n",
    "        # save the scores for the main figure\n",
    "        all_scores_200[i,n] = np.mean(delay_score)*100\n",
    "        \n",
    "# and it get saved so it is easier for the plotting ot just load it\n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\decoder_accuracy_all_neurons_SVM_200neurons.npy'])\n",
    "np.save(save_dir,all_scores_200) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94ea2c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:29<00:00,  7.26s/it]\n"
     ]
    }
   ],
   "source": [
    "# calculate decoding accuracy for different subpopulations of neurons and the decoding error\n",
    "# load the outputs from this code run_classifier_subpopulations.py\n",
    "\n",
    "n_reps = 20\n",
    "type_neurons = ['1','2','delay','3NOdelay']\n",
    "all_scores = np.full((len(type_neurons), n_reps), np.nan)\n",
    "all_errors = np.full((len(type_neurons), n_reps), np.nan)\n",
    "\n",
    "n = 200 # select how many neurons\n",
    "count = 1\n",
    "\n",
    "# put a blue colormap\n",
    "val_max = 100\n",
    "val_min = 0\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list('mycmap', [(1, 1, 1), '#000099'])\n",
    "    \n",
    "for mod in tqdm(type_neurons):\n",
    "    pred_50 =[]\n",
    "    test_50 = []\n",
    "    \n",
    "    file =''.join([data_path,f'decoder_datasets\\\\clf_types_neurons_SVM\\\\PRED_mod_{mod}_SVM.npy'])\n",
    "    all_predicted=np.load(file)\n",
    "    file =''.join([data_path,f'decoder_datasets\\\\clf_types_neurons_SVM\\\\TEST_mod_{mod}_SVM.npy'])\n",
    "    Y_test=np.load(file)\n",
    "\n",
    "    # extract values for a specific number of neurons    \n",
    "    if mod == 'delay':\n",
    "        pred = all_predicted[:,n,:,:]\n",
    "        test = Y_test[:,n,:,:]\n",
    "    elif mod == '3NOdelay':\n",
    "        pred = all_predicted[:,n,:,:]\n",
    "        test = Y_test[:,n,:,:]\n",
    "    else:\n",
    "        pred = all_predicted[count,:,n,:,:]\n",
    "        test = Y_test[count,:,n,:,:]    \n",
    "\n",
    "    for rep in range(pred.shape[0]): # for each repetition\n",
    "        delay_score = np.full((pred.shape[1],1), np.nan)\n",
    "        error_score = np.full((pred.shape[1],1), np.nan)\n",
    "        \n",
    "        for k in range(pred.shape[1]): # for each k fold\n",
    "            \n",
    "            # compute the confusion matrix\n",
    "            cm = confusion_matrix(test[rep,k,:], pred[rep,k,:],normalize='true')\n",
    "            \n",
    "            # extract the score for delays\n",
    "            score = np.diag(cm)\n",
    "            E_score = np.diag(cm[12:,:12])\n",
    "            if score.size > 1:\n",
    "                delay_score[k] = np.nanmean(score[:11]) \n",
    "                error_score[k] = np.nanmean(E_score) \n",
    "                    \n",
    "            \n",
    "        all_scores[count-1,rep] = np.nanmean(delay_score)*100\n",
    "        all_errors[count-1,rep] = np.nanmean(error_score)*100\n",
    "\n",
    "    count += 1\n",
    "    \n",
    "# and it get saved so it is easier for the plotting ot just load it\n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\decoder_accuracy_subpopulations_neurons_SVM.npy'])\n",
    "np.save(save_dir,all_scores)    \n",
    "\n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\decoder_error_subpopulations_neurons_SVM.npy'])\n",
    "np.save(save_dir,all_errors)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4749e224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "#  calculate decoding accuracy for decoder trained without Ashifted trials\n",
    "# load the outputs from this code run_classifier_subpopulations_submatrix.py\n",
    "\n",
    "n_reps = 20\n",
    "type_neurons = ['delay','3NOdelay']\n",
    "n = 200\n",
    "mean_error = np.zeros((n_reps,len(type_neurons)))\n",
    "\n",
    "for count,mod in enumerate(tqdm(type_neurons)):\n",
    "    \n",
    "    file =''.join([data_path,f'decoder_datasets\\\\clf_types_neurons_SVM\\\\PRED_mod_{mod}_SVM_small.npy'])\n",
    "    all_predicted=np.load(file)\n",
    "    file =''.join([data_path,f'decoder_datasets\\\\clf_types_neurons_SVM\\\\TEST_mod_{mod}_SVM_small.npy'])\n",
    "    Y_test=np.load(file)\n",
    "\n",
    "    # extract values for a specific number of neurons    \n",
    "    pred = all_predicted[:,n,:,:]\n",
    "    test = Y_test[:,n,:,:]\n",
    "    \n",
    "    for rep in range(pred.shape[0]):\n",
    "        \n",
    "        this_pred = pred[rep,:,:].reshape(-1)\n",
    "        this_test = test[rep,:,:].reshape(-1)\n",
    "        non_zero_error = ((this_test-this_pred).T)*10\n",
    "        # any error\n",
    "        mean_error[rep,count] = np.mean(abs(non_zero_error))\n",
    "        \n",
    "    count += 1\n",
    "    \n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\decoder_decoding_error_subpopulations_neurons_SVM_small.npy'])\n",
    "np.save(save_dir,mean_error) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2068f723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['AP_lim', 'ML_lim', 'all_boot_aud', 'all_boot_vis', 'animal_ID', 'binSize', 'coord3D', 'depth_lim', 'experiment_ID', 'modality', 'peaks', 'pvals', 'resp', 'spikes', 'trials', 'window_spikes'])\n"
     ]
    }
   ],
   "source": [
    "# import relevant datasets\n",
    "\n",
    "# load the main dataset\n",
    "file= ''.join([data_path,'neurons_datasets\\\\delay_tuning_dataset.mat'])\n",
    "data_dict = mat73.loadmat(file)\n",
    "DAT=data_dict['delay_tuning_dataset']\n",
    "\n",
    "# check keys available\n",
    "print(DAT.keys())\n",
    "\n",
    "# extract all keys\n",
    "for k in DAT.keys():\n",
    "    globals()[k] = DAT[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc73812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with all properties of the neurons that are used in the decoder\n",
    "\n",
    "# now let's extract the porperties we are looking for\n",
    "# delay selectivity index\n",
    "\n",
    "multi_peaks = peaks[:,:-2]    \n",
    "DSI = []\n",
    "for i in range(multi_peaks.shape[0]):\n",
    "    DSI.append((np.nanmax(multi_peaks[i,:])-np.nanmin(multi_peaks[i,:]))/(np.nanmax(multi_peaks[i,:])+np.nanmin(multi_peaks[i,:])))\n",
    "DSI = np.array(DSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4191ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. reliability of the responses to the preferred delay \n",
    "\n",
    "# load the reliability of the responses to the preferred delay \n",
    "load_dir = ''.join([data_path,'neurons_datasets\\\\Inter_trial_variability_neurons.npy'])\n",
    "final_corr = np.load(load_dir)\n",
    "pref_delay = np.argmax(multi_peaks,axis=1)\n",
    "RI = final_corr[np.arange(final_corr.shape[0]),pref_delay]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b893672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "its correct\n"
     ]
    }
   ],
   "source": [
    "# multisensory index\n",
    "# get spikes and labels in the right shape \n",
    "all_spikes_sub_mean, all_labels = shift_sum(spikes,trials,sub_mean=1)\n",
    "\n",
    "# reshape it in a two dimensional array \n",
    "all_spikes_reshaped = all_spikes_sub_mean.reshape(all_spikes_sub_mean.shape[0],-1)\n",
    "all_spikes_reshaped = all_spikes_reshaped.reshape(all_spikes_sub_mean.shape[0], -1, 50, all_spikes_sub_mean.shape[2])\n",
    "# Compute the mean along the second axis\n",
    "mean_array = np.mean(all_spikes_reshaped, axis=2)\n",
    "n_neurons = mean_array.shape[0]\n",
    "binSize = 0.01\n",
    "# find the peaks of the response\n",
    "peaks_all = np.zeros((n_neurons,mean_array.shape[1])) # N x T\n",
    "\n",
    "for n in range(n_neurons): # random order of trial for each neuron \n",
    "    \n",
    "    this_neuron_spikes = mean_array[n,:,2:]    \n",
    "    \n",
    "    # find the peakFR\n",
    "    max_pos = np.argmax(this_neuron_spikes,axis=1) \n",
    "    \n",
    "    for r in range(this_neuron_spikes.shape[0]):\n",
    "        peaks_all[n,r] = this_neuron_spikes[r,max_pos[r]]/binSize\n",
    "\n",
    "# get MII\n",
    "MII_all = np.zeros((peaks_all.shape[0],peaks.shape[1]-2))\n",
    "\n",
    "for n in range(peaks_all.shape[0]):\n",
    "    for d in range(11):\n",
    "        del_FR = peaks_all[n,d]\n",
    "        sum_FR = peaks_all[n,13+d]\n",
    "        sum_FR_old = peaks_all[n,13-2]+peaks_all[n,13-1]\n",
    "\n",
    "        if sum_FR <= 0 or del_FR <= 0:\n",
    "            MII_all[n,d] = np.nan\n",
    "        else:\n",
    "            MII_all[n,d] = (del_FR - sum_FR)/sum_FR\n",
    "\n",
    "MII_pref = MII_all[np.arange(final_corr.shape[0]),pref_delay]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41c7faca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specificity index \n",
    "\n",
    "how_many=[]\n",
    "for i in range(peaks.shape[0]):\n",
    "    \n",
    "    vis_FR = peaks[i,-2]\n",
    "    aud_FR = peaks[i,-1]\n",
    "    \n",
    "    if vis_FR>aud_FR:\n",
    "        boot_out = all_boot_vis[i,:]\n",
    "    elif aud_FR>vis_FR:\n",
    "        boot_out = all_boot_aud[i,:]\n",
    "    \n",
    "    pos_sig = np.argwhere(boot_out>0)\n",
    "    \n",
    "    if len(pos_sig)>0:\n",
    "        how_many.append(np.sum(boot_out[boot_out>0]))\n",
    "    else:\n",
    "        how_many.append(0)\n",
    "        \n",
    "unique_categories, counts = np.unique(how_many, return_counts=True)\n",
    "how_many = 1-(np.array(how_many)/11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f03352fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now save the ids for 25 neurons, for each rep\n",
    "# load the outputs from this code run_classifier_single_recordings.py\n",
    "\n",
    "n_neurons = 25\n",
    "n_rep = 20\n",
    "all_ids_recs = []\n",
    "recs = np.unique(experiment_ID).astype(int)\n",
    "# get how many neurons I have per rec\n",
    "rec_counts = {rec: np.sum(experiment_ID == rec) for rec in recs}\n",
    "# Loop over the specified range of `rec` values\n",
    "for loc, rec in enumerate(recs):\n",
    "    file_path = ''.join([data_path,f'decoder_datasets\\\\clf_single_rec\\\\rec{rec}_updated_ids_tracking_SVM.pkl'])\n",
    "    with open(file_path, 'rb') as f:\n",
    "        ids_tracking = pickle.load(f) \n",
    "    \n",
    "    if rec_counts[rec]<= n_neurons:   \n",
    "        \n",
    "        continue\n",
    "    curr_ids = np.zeros((n_neurons+1,n_rep))\n",
    "    for rep in range(n_rep):\n",
    "        curr_ids[:,rep] = ids_tracking[rep][:][n_neurons]\n",
    "            \n",
    "    all_ids_recs.append(curr_ids)\n",
    "all_ids_recs = np.array(all_ids_recs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e2f4b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 92/92 [00:18<00:00,  4.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# now let's get in the same size the accuracy of the decoder\n",
    "# load the outputs from this code run_classifier_single_recordings.py\n",
    "\n",
    "recs = np.unique(experiment_ID).astype(int)\n",
    "all_results = np.zeros((len(recs),20,1))\n",
    "\n",
    "# load each recording individually and store the mean accuracy \n",
    "for count, i in enumerate(tqdm(recs)):\n",
    "    file_path = ''.join([data_path,f'decoder_datasets\\\\clf_single_rec\\\\rec{i}_PRED_SVM.npy'])     \n",
    "    all_predicted=np.load(file_path)\n",
    "    file_path = ''.join([data_path,f'decoder_datasets\\\\clf_single_rec\\\\rec{i}_TEST_SVM.npy'])\n",
    "    Y_test=np.load(file_path)\n",
    " \n",
    "    if all_predicted.shape[1] <= n_neurons:\n",
    "        # skip this recording\n",
    "        continue\n",
    "\n",
    "    # also get the score for n_neurons\n",
    "    for rep in range(all_predicted.shape[0]): # for each repetition\n",
    "        \n",
    "        delay_score = np.zeros((all_predicted.shape[2],1))\n",
    "        for k in range(all_predicted.shape[2]): # for each kfold\n",
    "            pred = np.reshape(all_predicted[rep,n_neurons,k,:],-1)\n",
    "            test = np.reshape(Y_test[rep,n_neurons,k,:],-1)\n",
    "\n",
    "            cm = confusion_matrix(test,pred,normalize='true')\n",
    "            score = np.diag(cm)\n",
    "            delay_score[k] = np.mean(score[:11])\n",
    "            if delay_score[k] ==1:\n",
    "                delay_score[k] = 0\n",
    "\n",
    "        all_results[count,rep] = np.mean(delay_score)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43673548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how many delay neruons I have per recording\n",
    "\n",
    "sig_del = []\n",
    "which_tr = []\n",
    "for i in range(peaks.shape[0]):\n",
    "    y = peaks[i,:-2]\n",
    "\n",
    "    vis_FR = peaks[i,-2]\n",
    "    aud_FR = peaks[i,-1]\n",
    "\n",
    "    if vis_FR>aud_FR:\n",
    "        boot_out = all_boot_vis[i,:]\n",
    "    elif aud_FR>vis_FR:\n",
    "        boot_out = all_boot_aud[i,:]\n",
    "    \n",
    "    pos_sig = np.argwhere(boot_out>0)\n",
    "    \n",
    "    if len(pos_sig)>0:\n",
    "        sig_del.append(i)\n",
    "        tr = pos_sig[np.argmax(y[pos_sig])]\n",
    "        which_tr.append(tr) \n",
    "\n",
    "sig_del = np.array(sig_del)\n",
    "pref_delay=np.array(which_tr)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9800f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_accuracy = all_results[:, :, 0].flatten()\n",
    "pos = np.argwhere(flattened_accuracy == 0)\n",
    "\n",
    "# Create a mask to exclude positions with zeros\n",
    "mask = np.ones(flattened_accuracy.shape, dtype=bool)\n",
    "mask[pos] = False\n",
    "\n",
    "# Apply the mask to exclude zero positions\n",
    "filtered_accuracy = flattened_accuracy[mask]\n",
    "all_decoder_results = filtered_accuracy.reshape(-1,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b57f9347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize 3D coordinates\n",
    "actual_lengthML = ML_lim[1] - ML_lim[0]\n",
    "actual_lengthAP = AP_lim[1] - AP_lim[0]\n",
    "actual_lengthdepth = depth_lim[1] - depth_lim[0]\n",
    "\n",
    "# maybe it's better to take the actual length, so subtract the minimum of the range\n",
    "ML_norm = coord3D[:,2] - ML_lim[0]\n",
    "AP_norm = coord3D[:,0] - AP_lim[0]\n",
    "depth_norm = coord3D[:,1] - depth_lim[0]\n",
    "\n",
    "# invert the ML axis first\n",
    "\n",
    "max_val = actual_lengthML\n",
    "min_val = 0 \n",
    "new_ML = np.array([max_val - val + min_val for val in ML_norm])\n",
    "ML_norm = new_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c481e8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bianchg\\AppData\\Local\\Temp\\2\\ipykernel_6488\\1393190882.py:72: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  kurt = stats.kurtosis(this_var_no_nans)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset with all properties of the neurons that are used in the decoder\n",
    "\n",
    "# Initialize the array to store properties\n",
    "n_measures = 16\n",
    "n_recs = len(np.unique(experiment_ID[~np.isnan(AP_norm)]))\n",
    "n_reps = all_ids_recs.shape[2]\n",
    "all_ids_properties = []\n",
    "for this_rec in range(all_ids_recs.shape[0]):\n",
    "    rec_properties = []\n",
    "    for rep in range(all_ids_recs.shape[2]):\n",
    "        these_neurons = all_ids_recs[this_rec, :, rep].astype(int)\n",
    "        modality_here = modality[these_neurons]\n",
    "        modality_here[np.argwhere(modality_here>3)] = 3\n",
    "        \n",
    "        # let's give another value to delay neurons\n",
    "        common_values = np.intersect1d(sig_del, these_neurons)\n",
    "        positions = np.where(np.isin(these_neurons, common_values))[0]\n",
    "        modality_here[positions] = 4\n",
    "        \n",
    "        modality_int = modality_here.astype(int)\n",
    "\n",
    "        # Count occurrences using bincount\n",
    "        counts = np.bincount(modality_int)\n",
    "\n",
    "        # Create a dictionary to store the counts for each group (excluding zero)\n",
    "        group_counts = {i: count for i, count in enumerate(counts) if i > 0}\n",
    "        total_entries = modality_here.shape[0]\n",
    "\n",
    "        # Calculate proportions\n",
    "        proportions = {k: v / total_entries *100 for k, v in group_counts.items()}\n",
    " \n",
    "        # Check if any value in AP_norm[these_neurons] is NaN and skip if true\n",
    "        if np.isnan(AP_norm[these_neurons][0]):\n",
    "            continue\n",
    "        \n",
    "        # Collect properties for the current recording and repetition\n",
    "        properties = [\n",
    "            all_decoder_results[this_rec, rep],        # Accuracy\n",
    "            np.nanmean(DSI[these_neurons]),          # DSI\n",
    "            np.nanmedian(RI[these_neurons]),           # Reliability index\n",
    "            np.nanmedian(MII_pref[these_neurons]),     # MII\n",
    "            np.nanmedian(how_many[these_neurons]),     # Specificity index\n",
    "            np.nanmedian(experiment_ID[these_neurons]),# Experiment ID\n",
    "            np.nanmedian(animal_ID[these_neurons]), #Animal ID\n",
    "            np.nanmean(ML_norm[these_neurons]),      # Mean ML position\n",
    "            np.nanmean(AP_norm[these_neurons]),      # Mean AP position\n",
    "            np.nanmean(depth_norm[these_neurons]),    # Mean depth position\n",
    "            np.intersect1d(sig_del, these_neurons).shape[0]/25*100, # number delay neurons\n",
    "            proportions.get(1, 0), # visual neurons\n",
    "            proportions.get(2, 0), # auditory neurons\n",
    "            proportions.get(3, 0), #multisensory - no delay\n",
    "            proportions.get(4, 0), #delay neurons\n",
    "            np.nanmedian(np.abs(MII_pref[these_neurons])) \n",
    "        ]\n",
    "        \n",
    "        rec_properties.append(properties)\n",
    "\n",
    "    # Convert to numpy array for easier averaging\n",
    "    rec_properties = np.array(rec_properties)\n",
    "        \n",
    "    # Check if rec_properties is empty (due to skipping all reps), and skip if true\n",
    "    if rec_properties.size == 0:\n",
    "        continue\n",
    "        \n",
    "    # check the kurtosis of this distribution\n",
    "    mean_properties = []\n",
    "    \n",
    "    for m in range(rec_properties.shape[1]):\n",
    "        this_var = rec_properties[:,m]\n",
    "        this_var_no_nans =  this_var[~np.isnan(this_var)]\n",
    "        \n",
    "        kurt = stats.kurtosis(this_var_no_nans)\n",
    "        if m == 0:\n",
    "            mean_properties.append(np.nanmean(this_var))\n",
    "        elif m> 0:\n",
    "            if abs(kurt) <2:\n",
    "                mean_properties.append(np.nanmean(this_var))\n",
    "            else:\n",
    "                mean_properties.append(np.nanmedian(this_var))\n",
    "    \n",
    "    mean_properties = np.array(mean_properties)\n",
    "    \n",
    "    all_ids_properties.append(mean_properties)\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "all_ids_properties = np.array(all_ids_properties)\n",
    "\n",
    "# Reshape the array to the appropriate dimensions\n",
    "# Note: Here we only need n_recs and n_measures as we already averaged across repetitions\n",
    "all_ids_properties = all_ids_properties.reshape(-1, n_measures)\n",
    "\n",
    "\n",
    "# save it so you can test it on matlab \n",
    "data = {\n",
    "    'accuracy': all_ids_properties[:, 0].flatten(),\n",
    "    'DSI': all_ids_properties[:, 1].flatten(),\n",
    "    'reliability': all_ids_properties[:, 2].flatten(),\n",
    "    'MII': all_ids_properties[:, 3].flatten(),\n",
    "    'specificity': all_ids_properties[:, 4].flatten(),\n",
    "    'experiment_ID' : all_ids_properties[:, 5].flatten(),\n",
    "    'animal_ID' : all_ids_properties[:, 6].flatten(),\n",
    "    'ML' : all_ids_properties[:, 7].flatten(),\n",
    "    'AP' : all_ids_properties[:, 8].flatten(),\n",
    "    'depth' : all_ids_properties[:, 9].flatten(),\n",
    "    'n_delay' : all_ids_properties[:, 10].flatten(),\n",
    "    'n_vis' : all_ids_properties[:, 11].flatten(),\n",
    "    'n_aud' : all_ids_properties[:, 12].flatten(),\n",
    "    'n_multi' : all_ids_properties[:, 13].flatten(),\n",
    "    'n_delay2': all_ids_properties[:, 14].flatten(),\n",
    "    'abs_MII' : all_ids_properties[:, 15].flatten()\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Saving DataFrame to a CSV file\n",
    "df.to_csv(''.join([data_path,f'decoder_datasets\\\\data_all_mean_25n.csv'])  , index=False) \n",
    "\n",
    "print('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f85bdabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:38<00:00,  1.94s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:39<00:00,  1.99s/it]\n"
     ]
    }
   ],
   "source": [
    "# extract accuracy with synthetics summs and observed responses - when only using delay neurons\n",
    "# load the outputs from this code run_classifier_delay_neurons_only_linear_observed.py\n",
    "\n",
    "n_neurons = 662\n",
    "all_results_SVM_linear_delay = np.zeros((n_iter,n_neurons))   \n",
    "n_iter = 20\n",
    "# load each repetition infividually and store the mean accuracy \n",
    "for i in tqdm(range(n_iter)):\n",
    "    file_path = ''.join([data_path,f'decoder_datasets\\\\clf_increasing_N_SVM\\\\PRED_rep{i}_linear_SVM_delay.npy'])  \n",
    "    all_predicted=np.load(file_path)\n",
    "    file =''.join([data_path,f'decoder_datasets\\\\clf_increasing_N_SVM\\\\TEST_rep{i}_linear_SVM_delay.npy']) \n",
    "    Y_test=np.load(file)\n",
    "\n",
    "    pred = all_predicted[i,:,:,:]\n",
    "    test = Y_test[i,:,:,:]\n",
    "\n",
    "    for n in range(n_neurons): # for each neuron\n",
    "        delay_score = np.zeros((pred.shape[1],1))\n",
    "        for k in range(pred.shape[1]): # for each kfold\n",
    "            cm = confusion_matrix(test[n,k,:], pred[n,k,:],normalize='true')\n",
    "            # extract the score for delays\n",
    "            score = np.diag(cm)\n",
    "            delay_score[k] = np.mean(score[:11])                \n",
    "\n",
    "        # save the scores for the main figure\n",
    "        all_results_SVM_linear_delay [i,n] = np.mean(delay_score)*100\n",
    "\n",
    "# load the outputs from this code run_classifier_delay_neurons_only_linear_observed.py\n",
    "# initiate empty lists to append \n",
    "all_results_SVM_delay = np.zeros((n_iter,n_neurons))   \n",
    "\n",
    "# load each repetition infividually and store the mean accuracy \n",
    "for i in tqdm(range(n_iter)):\n",
    "    file_path = ''.join([data_path,f'decoder_datasets\\\\clf_increasing_N_SVM\\\\PRED_rep{i}_SVM_delay.npy'])  \n",
    "    all_predicted=np.load(file_path)\n",
    "    file =''.join([data_path,f'decoder_datasets\\\\clf_increasing_N_SVM\\\\TEST_rep{i}_SVM_delay.npy']) \n",
    "    Y_test=np.load(file)\n",
    "\n",
    "    pred = all_predicted[i,:,:,:]\n",
    "    test = Y_test[i,:,:,:]\n",
    "\n",
    "    for n in range(n_neurons): # for each neuron\n",
    "        delay_score = np.zeros((pred.shape[1],1))\n",
    "        for k in range(pred.shape[1]): # for each kfold\n",
    "            cm = confusion_matrix(test[n,k,:], pred[n,k,:],normalize='true')\n",
    "            # extract the score for delays\n",
    "            score = np.diag(cm)\n",
    "            delay_score[k] = np.mean(score[:11])                \n",
    "\n",
    "        # save the scores for the main figure\n",
    "        all_results_SVM_delay [i,n] = np.mean(delay_score)*100\n",
    "        \n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\decoder_accuracy_SVM_delay.npy'])\n",
    "np.save(save_dir,all_results_SVM_delay) \n",
    "\n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\decoder_accuracy_SVM_delay_linear.npy'])\n",
    "np.save(save_dir,all_results_SVM_linear_delay) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c61851cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [01:47<00:00,  5.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# extract accuracy with synthetics summs and observed responses - with an SVM\n",
    "# load the outputs from this code run_classifier_increasing_N_linear.py\n",
    "\n",
    "n_neurons = 662\n",
    "all_results_SVM_linear = np.zeros((n_iter,n_neurons))   \n",
    "n_iter = 20\n",
    "# load each repetition infividually and store the mean accuracy \n",
    "for i in tqdm(range(n_iter)):\n",
    "    file_path = ''.join([data_path,f'decoder_datasets\\\\clf_increasing_N_SVM_linear\\\\PRED_rep{i}_SVM.npy'])  \n",
    "    all_predicted=np.load(file_path)\n",
    "    file =''.join([data_path,f'decoder_datasets\\\\clf_increasing_N_SVM_linear\\\\TEST_rep{i}_SVM.npy']) \n",
    "    Y_test=np.load(file)\n",
    "\n",
    "    pred = all_predicted[i,:,:,:]\n",
    "    test = Y_test[i,:,:,:]\n",
    "\n",
    "    for n in range(n_neurons): # for each neuron\n",
    "        delay_score = np.zeros((pred.shape[1],1))\n",
    "        for k in range(pred.shape[1]): # for each kfold\n",
    "            cm = confusion_matrix(test[n,k,:], pred[n,k,:],normalize='true')\n",
    "            # extract the score for delays\n",
    "            score = np.diag(cm)\n",
    "            delay_score[k] = np.mean(score[:11])                \n",
    "\n",
    "        # save the scores for the main figure\n",
    "        all_results_SVM_linear [i,n] = np.mean(delay_score)*100\n",
    "\n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\decoder_accuracy_SVM_linear.npy'])\n",
    "np.save(save_dir,all_results_SVM_linear) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "257a940a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [01:46<00:00,  5.33s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [01:45<00:00,  5.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# extract accuracy with synthetics summs and observed responses - when only using delay neurons\n",
    "# load the outputs from this code run_classifier_increasing_N_linear.py\n",
    "\n",
    "n_neurons = 662\n",
    "all_results_RFC_linear = np.zeros((n_iter,n_neurons))   \n",
    "n_iter = 20\n",
    "# load each repetition infividually and store the mean accuracy \n",
    "for i in tqdm(range(n_iter)):\n",
    "    file_path = ''.join([data_path,f'decoder_datasets\\\\clf_increasing_N_RFC_linear\\\\PRED_rep{i}.npy'])  \n",
    "    all_predicted=np.load(file_path)\n",
    "    file =''.join([data_path,f'decoder_datasets\\\\clf_increasing_N_RFC_linear\\\\TEST_rep{i}.npy']) \n",
    "    Y_test=np.load(file)\n",
    "\n",
    "    pred = all_predicted[i,:,:,:]\n",
    "    test = Y_test[i,:,:,:]\n",
    "\n",
    "    for n in range(n_neurons): # for each neuron\n",
    "        delay_score = np.zeros((pred.shape[1],1))\n",
    "        for k in range(pred.shape[1]): # for each kfold\n",
    "            cm = confusion_matrix(test[n,k,:], pred[n,k,:],normalize='true')\n",
    "            # extract the score for delays\n",
    "            score = np.diag(cm)\n",
    "            delay_score[k] = np.mean(score[:11])                \n",
    "\n",
    "        # save the scores for the main figure\n",
    "        all_results_RFC_linear [i,n] = np.mean(delay_score)*100\n",
    "\n",
    "# load the outputs from this code run_classifier_increasing_N.py\n",
    "# initiate empty lists to append \n",
    "all_results_RFC = np.zeros((n_iter,n_neurons))   \n",
    "\n",
    "# load each repetition infividually and store the mean accuracy \n",
    "for i in tqdm(range(n_iter)):\n",
    "    file_path = ''.join([data_path,f'decoder_datasets\\\\clf_increasing_N_RFC\\\\PRED_rep{i}.npy'])  \n",
    "    all_predicted=np.load(file_path)\n",
    "    file =''.join([data_path,f'decoder_datasets\\\\clf_increasing_N_RFC\\\\TEST_rep{i}.npy']) \n",
    "    Y_test=np.load(file)\n",
    "\n",
    "    pred = all_predicted[i,:,:,:]\n",
    "    test = Y_test[i,:,:,:]\n",
    "\n",
    "    for n in range(n_neurons): # for each neuron\n",
    "        delay_score = np.zeros((pred.shape[1],1))\n",
    "        for k in range(pred.shape[1]): # for each kfold\n",
    "            cm = confusion_matrix(test[n,k,:], pred[n,k,:],normalize='true')\n",
    "            # extract the score for delays\n",
    "            score = np.diag(cm)\n",
    "            delay_score[k] = np.mean(score[:11])                \n",
    "\n",
    "        # save the scores for the main figure\n",
    "        all_results_RFC [i,n] = np.mean(delay_score)*100\n",
    "        \n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\decoder_accuracy_RFC.npy'])\n",
    "np.save(save_dir,all_results_RFC) \n",
    "\n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\decoder_accuracy_RFC_linear.npy'])\n",
    "np.save(save_dir,all_results_RFC_linear) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "169e576b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 92/92 [00:27<00:00,  3.30it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 92/92 [00:27<00:00,  3.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# extract accuracy with synthetics summs and observed responses - SVM\n",
    "# load the outputs from this code run_classifier_single_recordings_linear.py\n",
    "\n",
    "recs = np.unique(experiment_ID).astype(int)\n",
    "\n",
    "all_results_SVM_linear = np.zeros((len(recs),20,1))\n",
    "\n",
    "# load each recording individually and store the mean accuracy \n",
    "for count, i in enumerate(tqdm(recs)):\n",
    "    file_path = ''.join([data_path,f'decoder_datasets\\\\clf_single_rec\\\\rec{i}_PRED_linear_SVM.npy'])     \n",
    "    all_predicted=np.load(file_path)\n",
    "    file_path = ''.join([data_path,f'decoder_datasets\\\\clf_single_rec\\\\rec{i}_TEST_linear_SVM.npy'])  \n",
    "    Y_test=np.load(file_path)\n",
    "    max_n = all_predicted.shape[1]-1\n",
    "\n",
    "    # also get the score \n",
    "    for rep in range(all_predicted.shape[0]): # for each repetition\n",
    "        delay_score = np.zeros((all_predicted.shape[2],1))\n",
    "        for k in range(all_predicted.shape[2]): # for each kfold\n",
    "            pred = np.reshape(all_predicted[rep,max_n,k,:],-1)\n",
    "            test = np.reshape(Y_test[rep,max_n,k,:],-1)\n",
    "\n",
    "            cm = confusion_matrix(test,pred,normalize='true')\n",
    "            score = np.diag(cm)\n",
    "            delay_score[k] = np.mean(score[:11])\n",
    "\n",
    "            if delay_score[k] ==1:\n",
    "                delay_score[k] = NaN\n",
    "\n",
    "        all_results_SVM_linear[count,rep] = np.mean(delay_score)*100\n",
    "\n",
    "# load the outputs from this code run_classifier_single_recordings.py\n",
    "all_results_SVM = np.zeros((len(recs),20,1))\n",
    "number_neurons = np.zeros((len(recs),1))\n",
    "\n",
    "# load each recording individually and store the mean accuracy \n",
    "for count, i in enumerate(tqdm(recs)):\n",
    "    file_path = ''.join([data_path,f'decoder_datasets\\\\clf_single_rec\\\\rec{i}_PRED_SVM.npy'])      \n",
    "    all_predicted=np.load(file_path)\n",
    "    file_path = ''.join([data_path,f'decoder_datasets\\\\clf_single_rec\\\\rec{i}_TEST_SVM.npy'])  \n",
    "    Y_test=np.load(file_path)\n",
    "    max_n = all_predicted.shape[1]-1\n",
    "    number_neurons[count] = max_n\n",
    "    \n",
    "    # also get the score \n",
    "    for rep in range(all_predicted.shape[0]): # for each repetition\n",
    "        delay_score = np.zeros((all_predicted.shape[2],1))\n",
    "        for k in range(all_predicted.shape[2]): # for each kfold\n",
    "            pred = np.reshape(all_predicted[rep,max_n,k,:],-1)\n",
    "            test = np.reshape(Y_test[rep,max_n,k,:],-1)\n",
    "\n",
    "            cm = confusion_matrix(test,pred,normalize='true')\n",
    "            score = np.diag(cm)\n",
    "            delay_score[k] = np.mean(score[:11])\n",
    "\n",
    "            if delay_score[k] ==1:\n",
    "                delay_score[k] = NaN\n",
    "\n",
    "        all_results_SVM[count,rep] = np.mean(delay_score)*100\n",
    "        \n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\decoder_accuracy_single_rec_SVM.npy'])\n",
    "np.save(save_dir,all_results_SVM) \n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\decoder_accuracy_single_rec_SVM_numberN.npy'])\n",
    "np.save(save_dir,number_neurons) \n",
    "\n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\decoder_accuracy_single_rec_SVM_linear.npy'])\n",
    "np.save(save_dir,all_results_SVM_linear) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7364b731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 92/92 [00:17<00:00,  5.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# now let's get in the same size the accuracy of the decoder\n",
    "# load the outputs from this code run_classifier_single_recordings.py\n",
    "\n",
    "recs = np.unique(experiment_ID).astype(int)\n",
    "all_results_ind = np.zeros((len(recs),20,1))\n",
    "n_neurons = 25\n",
    "\n",
    "# load each recording individually and store the mean accuracy \n",
    "for count, i in enumerate(tqdm(recs)):\n",
    "    file_path = ''.join([data_path,f'decoder_datasets\\\\clf_single_rec\\\\rec{i}_PRED_SVM.npy'])     \n",
    "    all_predicted=np.load(file_path)\n",
    "    file_path = ''.join([data_path,f'decoder_datasets\\\\clf_single_rec\\\\rec{i}_TEST_SVM.npy'])\n",
    "    Y_test=np.load(file_path)\n",
    " \n",
    "    if all_predicted.shape[1] <= n_neurons:\n",
    "        # skip this recording\n",
    "        continue\n",
    "\n",
    "    # also get the score for n_neurons\n",
    "    for rep in range(all_predicted.shape[0]): # for each repetition\n",
    "        \n",
    "        delay_score = np.zeros((all_predicted.shape[2],1))\n",
    "        for k in range(all_predicted.shape[2]): # for each kfold\n",
    "            pred = np.reshape(all_predicted[rep,n_neurons,k,:],-1)\n",
    "            test = np.reshape(Y_test[rep,n_neurons,k,:],-1)\n",
    "\n",
    "            cm = confusion_matrix(test,pred,normalize='true')\n",
    "            score = np.diag(cm)\n",
    "            delay_score[k] = np.mean(score[:11])\n",
    "            if delay_score[k] ==1:\n",
    "                delay_score[k] = 0\n",
    "\n",
    "        all_results_ind[count,rep] = np.mean(delay_score)*100\n",
    "        \n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\decoder_accuracy_single_rec_SVM_25N.npy'])\n",
    "np.save(save_dir,all_results_ind) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4df0b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the locations of these recordings for plotting purposes\n",
    "\n",
    "# load accuracy of individual recordings\n",
    "load_dir = ''.join([data_path,'decoder_datasets\\\\decoder_accuracy_single_rec_SVM_25N.npy'])\n",
    "accuracy_SVM= np.load(load_dir) \n",
    "accuracy_SVM = np.mean(accuracy_SVM,axis=1)\n",
    "\n",
    "# get the mean ML and AP position ofr each recording session\n",
    "experiment_ID = DAT['experiment_ID']\n",
    "recs = np.unique(experiment_ID).astype(int)\n",
    "mean_ML = np.zeros((accuracy_SVM.shape[0],1))\n",
    "mean_AP = np.zeros((accuracy_SVM.shape[0],1))\n",
    "for count, i in enumerate(recs):\n",
    "    pos = np.argwhere(experiment_ID == i)\n",
    "    mean_ML[count] = np.nanmean(ML_norm[pos])\n",
    "    mean_AP[count] = np.nanmean(AP_norm[pos])\n",
    "\n",
    "val_min = 0 \n",
    "val_max = np.round(np.nanmax(accuracy_SVM)).astype(int)\n",
    "big_val_max = val_max\n",
    "\n",
    "# and it get saved so it is easier for the plotting ot just load it\n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\decoder_accuracy_single_rec_SVM_25N_spatial_coord.npz'])\n",
    "np.savez(save_dir, mean_ML=mean_ML, mean_AP=mean_AP, accuracy_SVM=accuracy_SVM,actual_lengthML=actual_lengthML,actual_lengthAP=actual_lengthAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "036e05e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 92/92 [00:23<00:00,  3.95it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 92/92 [00:25<00:00,  3.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# extract accuracy with synthetics summs and observed responses - RFC\n",
    "# load the outputs from this code run_classifier_single_recordings_linear.py\n",
    "\n",
    "recs = np.unique(experiment_ID).astype(int)\n",
    "\n",
    "all_results_SVM_linear = np.zeros((len(recs),20,1))\n",
    "\n",
    "# load each recording individually and store the mean accuracy \n",
    "for count, i in enumerate(tqdm(recs)):\n",
    "    file_path = ''.join([data_path,f'decoder_datasets\\\\clf_single_rec\\\\rec{i}_PRED_linear.npy'])     \n",
    "    all_predicted=np.load(file_path)\n",
    "    file_path = ''.join([data_path,f'decoder_datasets\\\\clf_single_rec\\\\rec{i}_TEST_linear.npy'])  \n",
    "    Y_test=np.load(file_path)\n",
    "    max_n = all_predicted.shape[1]-1\n",
    "\n",
    "    # also get the score \n",
    "    for rep in range(all_predicted.shape[0]): # for each repetition\n",
    "        delay_score = np.zeros((all_predicted.shape[2],1))\n",
    "        for k in range(all_predicted.shape[2]): # for each kfold\n",
    "            pred = np.reshape(all_predicted[rep,max_n,k,:],-1)\n",
    "            test = np.reshape(Y_test[rep,max_n,k,:],-1)\n",
    "\n",
    "            cm = confusion_matrix(test,pred,normalize='true')\n",
    "            score = np.diag(cm)\n",
    "            delay_score[k] = np.mean(score[:11])\n",
    "\n",
    "            if delay_score[k] ==1:\n",
    "                delay_score[k] = NaN\n",
    "\n",
    "        all_results_SVM_linear[count,rep] = np.mean(delay_score)*100\n",
    "\n",
    "# load the outputs from this code run_classifier_single_recordings.py\n",
    "all_results_SVM = np.zeros((len(recs),20,1))\n",
    "\n",
    "# load each recording individually and store the mean accuracy \n",
    "for count, i in enumerate(tqdm(recs)):\n",
    "    file_path = ''.join([data_path,f'decoder_datasets\\\\clf_single_rec\\\\rec{i}_PRED.npy'])      \n",
    "    all_predicted=np.load(file_path)\n",
    "    file_path = ''.join([data_path,f'decoder_datasets\\\\clf_single_rec\\\\rec{i}_TEST.npy'])  \n",
    "    Y_test=np.load(file_path)\n",
    "    max_n = all_predicted.shape[1]-1\n",
    "\n",
    "    # also get the score \n",
    "    for rep in range(all_predicted.shape[0]): # for each repetition\n",
    "        delay_score = np.zeros((all_predicted.shape[2],1))\n",
    "        for k in range(all_predicted.shape[2]): # for each kfold\n",
    "            pred = np.reshape(all_predicted[rep,max_n,k,:],-1)\n",
    "            test = np.reshape(Y_test[rep,max_n,k,:],-1)\n",
    "\n",
    "            cm = confusion_matrix(test,pred,normalize='true')\n",
    "            score = np.diag(cm)\n",
    "            delay_score[k] = np.mean(score[:11])\n",
    "\n",
    "            if delay_score[k] ==1:\n",
    "                delay_score[k] = NaN\n",
    "\n",
    "        all_results_SVM[count,rep] = np.mean(delay_score)*100\n",
    "        \n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\decoder_accuracy_single_rec_RFC.npy'])\n",
    "np.save(save_dir,all_results_SVM) \n",
    "\n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\decoder_accuracy_single_rec_RFC_linear.npy'])\n",
    "np.save(save_dir,all_results_SVM_linear) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "469cecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save deocding accuracy along anatomical axes for both linear and observed\n",
    "# load the outputs from this code run_classifier_1D.py\n",
    "\n",
    "axes =['ML','AP','depth']\n",
    "n = 50 # number of neurons\n",
    "r = 650 # bin size\n",
    "n_rep = 20\n",
    "n_axes = 3\n",
    "n_bins = 4\n",
    "\n",
    "scores = np.zeros((n_axes,2,n_rep,n_bins))\n",
    "\n",
    "# for observed\n",
    "for count,type_axis in enumerate(axes):\n",
    "    \n",
    "    # load the predicted and the test\n",
    "    file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_PRED_n{n}_step{r}_SVM.npy'])\n",
    "    all_predicted = np.load(file)\n",
    "    file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_TEST_n{n}_step{r}_SVM.npy'])\n",
    "    Y_test=np.load(file)\n",
    "\n",
    "    # load bin specs\n",
    "    file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_IDS_n{n}_step{r}_SVM.npy']) # the ids of the neurons\n",
    "    specs_neurons = np.load(file)\n",
    "    file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_BINS_n{n}_step{r}_SVM.npy']) # the ids of the neurons # the number of session and animals in the bin \n",
    "    specs_bins = np.load(file)\n",
    "    loc = specs_bins[:,0]\n",
    "\n",
    "    for i in range(1,len(loc)):\n",
    "        loc[i]= loc[i-1]+loc[i]\n",
    "    locs = np.zeros((2,len(loc)))\n",
    "    for i in range(len(loc)):\n",
    "        if i==0:\n",
    "            locs[:,i] = [0,loc[i]]\n",
    "        else:\n",
    "            locs[:,i] = [loc[i-1],loc[i]]\n",
    "\n",
    "\n",
    "    for n_bin in range(all_predicted.shape[0]): # for each bin\n",
    "        pred = all_predicted[n_bin,:,:,:]\n",
    "        test = Y_test[n_bin,:,:,:]\n",
    "     \n",
    "        for rep in range(all_predicted.shape[1]): #for each repetition\n",
    "            delay_score = np.zeros((all_predicted.shape[2],1))\n",
    "            for k in range(all_predicted.shape[2]): # for each k fold\n",
    "            # now plot the confusion matrix    \n",
    "                cm = confusion_matrix(test[rep,k,:], pred[rep,k,:],normalize='true')\n",
    "                score = np.diag(cm)\n",
    "                delay_score[k] = np.mean(score[:11])\n",
    "                if delay_score[k] ==1:\n",
    "                    delay_score[k] = float(\"nan\")\n",
    "            scores[count,0,rep,n_bin] = np.mean(delay_score)*100\n",
    "\n",
    "# for linear\n",
    "# load the outputs from this code run_classifier_1D_linear.py\n",
    "\n",
    "for count,type_axis in enumerate(axes):\n",
    "    \n",
    "    # load the predicted and the test\n",
    "    file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_PRED_n{n}_step{r}_SVM_linear.npy'])\n",
    "    all_predicted = np.load(file)\n",
    "    file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_TEST_n{n}_step{r}_SVM_linear.npy'])\n",
    "    Y_test=np.load(file)\n",
    "\n",
    "    # load bin specs\n",
    "    file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_BINS_n{n}_step{r}_SVM_linear.npy']) # the ids of the neurons # the number of session and animals in the bin \n",
    "    specs_bins = np.load(file)\n",
    "    loc = specs_bins[:,0]\n",
    "\n",
    "    for i in range(1,len(loc)):\n",
    "        loc[i]= loc[i-1]+loc[i]\n",
    "    locs = np.zeros((2,len(loc)))\n",
    "    for i in range(len(loc)):\n",
    "        if i==0:\n",
    "            locs[:,i] = [0,loc[i]]\n",
    "        else:\n",
    "            locs[:,i] = [loc[i-1],loc[i]]\n",
    "\n",
    "\n",
    "    for n_bin in range(all_predicted.shape[0]): # for each bin\n",
    "        pred = all_predicted[n_bin,:,:,:]\n",
    "        test = Y_test[n_bin,:,:,:]\n",
    "     \n",
    "        for rep in range(all_predicted.shape[1]): #for each repetition\n",
    "            delay_score = np.zeros((all_predicted.shape[2],1))\n",
    "            for k in range(all_predicted.shape[2]): # for each k fold\n",
    "            # now plot the confusion matrix    \n",
    "                cm = confusion_matrix(test[rep,k,:], pred[rep,k,:],normalize='true')\n",
    "                score = np.diag(cm)\n",
    "                delay_score[k] = np.mean(score[:11])\n",
    "                if delay_score[k] ==1:\n",
    "                    delay_score[k] = float(\"nan\")\n",
    "            scores[count,1,rep,n_bin] = np.mean(delay_score)*100\n",
    "\n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\decoder_accuracy_axes_1D.npy'])\n",
    "np.save(save_dir,scores) \n",
    "\n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\decoder_accuracy_axes_1D_locations.npy'])\n",
    "np.save(save_dir,locs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "753d90c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:40<00:00, 13.45s/it]\n"
     ]
    }
   ],
   "source": [
    "#Extract pvalues for classifier in 1d - varying the bin size and number of neurons\n",
    "# load the outputs from this code run_classifier_1D.py\n",
    "\n",
    "n_neurons = [25,50,75,100] \n",
    "range_length = np.arange(250,750,50)\n",
    "\n",
    "axes = ['ML','AP','depth']\n",
    "count = 0\n",
    "all_p = np.full((len(axes), len(n_neurons), len(range_length)), np.nan)\n",
    "for type_axis in tqdm(axes):\n",
    "    #extract all pvalues   \n",
    "    for rx in range(len(range_length)):  \n",
    "        r = range_length[rx]\n",
    "        for nx in range(len(n_neurons)):\n",
    "            n = n_neurons[nx]\n",
    "            file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_PRED_n{n}_step{r}_SVM.npy'])\n",
    "            all_predicted = np.load(file)\n",
    "            file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_TEST_n{n}_step{r}_SVM.npy'])\n",
    "            Y_test=np.load(file)\n",
    "        \n",
    "            scores = np.zeros((all_predicted.shape[1],all_predicted.shape[0]))    \n",
    "            for n_bin in range(all_predicted.shape[0]): # for each bin            \n",
    "                for rep in range(all_predicted.shape[1]): #for each repetition\n",
    "                    delay_score = np.zeros((all_predicted.shape[2],1))\n",
    "                    for k in range(all_predicted.shape[2]): # for each k fold\n",
    "                        # now plot the confusion matrix    \n",
    "                        cm = confusion_matrix(Y_test[n_bin,rep,k,:], all_predicted[n_bin,rep,k,:],normalize='true')\n",
    "                        score = np.diag(cm)\n",
    "                        delay_score[k] = np.mean(score[:11])\n",
    "                        \n",
    "                        if delay_score[k] ==1:\n",
    "                            delay_score[k] = float(\"nan\")\n",
    "                    scores[rep,n_bin] = np.mean(delay_score)*100\n",
    "       \n",
    "            # perform Kruskal-Wallis test\n",
    "            stat, all_p[count,nx,rx] = kruskal_test(*scores.T)\n",
    "    count +=1\n",
    "    \n",
    "save_dir = ''.join([data_path,'decoder_datasets\\\\pvalues_accuracy_axes_1D.npy'])\n",
    "np.save(save_dir,all_p)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1ea13eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract decoding accuracy for classifier in 1d varying the number of neurons for fixed bins size\n",
    "# load the outputs from this code run_classifier_1D.py\n",
    "\n",
    "axes = ['ML','AP','depth']\n",
    "\n",
    "n_neurons = [25,50,75,100]\n",
    "this_length = 650\n",
    "\n",
    "for type_axis in axes:\n",
    "    for nx in range(len(n_neurons)):\n",
    "        n = n_neurons[nx]\n",
    "        file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_PRED_n{n}_step{this_length}_SVM.npy'])\n",
    "        all_predicted = np.load(file)\n",
    "        file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_TEST_n{n}_step{this_length}_SVM.npy'])\n",
    "        Y_test=np.load(file)\n",
    "\n",
    "        # load bin specs\n",
    "        file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_IDS_n{n}_step{this_length}_SVM.npy']) # the ids of the neurons\n",
    "        specs_neurons = np.load(file)\n",
    "        file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_BINS_n{n}_step{this_length}_SVM.npy']) # the ids of the neurons # the number of session and animals in the bin \n",
    "        specs_bins = np.load(file)\n",
    "        loc = specs_bins[:,0]\n",
    "        for i in range(1,len(loc)):\n",
    "            loc[i]= loc[i-1]+loc[i]\n",
    "\n",
    "        locs = np.zeros((2,len(loc)))\n",
    "        for i in range(len(loc)):\n",
    "            if i==0:\n",
    "                locs[:,i] = [0,loc[i]]\n",
    "            else:\n",
    "                locs[:,i] = [loc[i-1],loc[i]]\n",
    "\n",
    "        scores2 = np.zeros((all_predicted.shape[1],all_predicted.shape[0]))\n",
    "\n",
    "        for n_bin in range(all_predicted.shape[0]): # for each bin\n",
    "            pred = all_predicted[n_bin,:,:,:]\n",
    "            test = Y_test[n_bin,:,:,:]\n",
    "\n",
    "            for rep in range(all_predicted.shape[1]): #for each repetition\n",
    "                delay_score = np.zeros((all_predicted.shape[2],1))\n",
    "                for k in range(all_predicted.shape[2]): # for each k fold\n",
    "                # now plot the confusion matrix    \n",
    "                    cm = confusion_matrix(test[rep,k,:], pred[rep,k,:],normalize='true')\n",
    "                    score = np.diag(cm)\n",
    "                    delay_score[k] = np.mean(score[:11])\n",
    "                    if delay_score[k] ==1:\n",
    "                        delay_score[k] = float(\"nan\")\n",
    "                scores2[rep,n_bin] = np.mean(delay_score)*100\n",
    "\n",
    "        save_dir = ''.join([data_path,f'decoder_datasets\\\\decoder_accuracy_{type_axis}_650bins_{n}N.npy'])\n",
    "        np.save(save_dir,scores2) \n",
    "\n",
    "        save_dir = ''.join([data_path,f'decoder_datasets\\\\decoder_accuracy_{type_axis}_locations_650bins_{n}N.npy'])\n",
    "        np.save(save_dir,locs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4b89bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding accuracy for 50 neurons and varying bin size\n",
    "# load the outputs from this code run_classifier_1D.py\n",
    "\n",
    "this_n_neurons = 50\n",
    "range_length = np.arange(250,750,100)\n",
    "this=0\n",
    "\n",
    "for type_axis in axes:\n",
    "    for rx in range(len(range_length)):  \n",
    "        # plot a figure with all the neurons possibilities\n",
    "        # load the data\n",
    "        r = range_length[rx]\n",
    "        file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_PRED_n{this_n_neurons}_step{r}_SVM.npy'])\n",
    "        all_predicted = np.load(file)\n",
    "        file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_TEST_n{this_n_neurons}_step{r}_SVM.npy'])\n",
    "        Y_test=np.load(file)\n",
    "        # load bin specs\n",
    "        file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_IDS_n{this_n_neurons}_step{r}_SVM.npy']) # the ids of the neurons\n",
    "        specs_neurons = np.load(file)\n",
    "        file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_BINS_n{this_n_neurons}_step{r}_SVM.npy']) # the ids of the neurons # the number of session and animals in the bin \n",
    "        specs_bins = np.load(file)\n",
    "\n",
    "        loc = specs_bins[:,0]    \n",
    "\n",
    "        cumulative_locs = np.cumsum(loc)\n",
    "        locs = np.copy(cumulative_locs)\n",
    "        for i in range(len(loc)):\n",
    "            if i==0:\n",
    "                locs[i] = cumulative_locs[i]/2\n",
    "            else:\n",
    "                locs[i] = cumulative_locs[i] - (cumulative_locs[i]-cumulative_locs[i-1])/2\n",
    "\n",
    "        scores = np.zeros((all_predicted.shape[1],all_predicted.shape[0]))\n",
    "\n",
    "        for n_bin in range(all_predicted.shape[0]): # for each bin\n",
    "            pred = all_predicted[n_bin,:,:,:]\n",
    "            test = Y_test[n_bin,:,:,:]\n",
    "\n",
    "            for rep in range(all_predicted.shape[1]): #for each repetition\n",
    "                delay_score = np.zeros((all_predicted.shape[2],1))\n",
    "                for k in range(all_predicted.shape[2]): # for each k fold\n",
    "\n",
    "                    # now plot the confusion matrix    \n",
    "                    cm = confusion_matrix(test[rep,k,:], pred[rep,k,:],normalize='true')\n",
    "                    score = np.diag(cm)\n",
    "                    delay_score[k] = np.mean(score[:11])\n",
    "                    if delay_score[k] ==1:\n",
    "                        delay_score[k] = float(\"nan\")\n",
    "                scores[rep,n_bin] = np.mean(delay_score)*100\n",
    "                \n",
    "        save_dir = ''.join([data_path,f'decoder_datasets\\\\decoder_accuracy_{type_axis}_{r}B_50N.npy'])\n",
    "        np.save(save_dir,scores) \n",
    "\n",
    "        save_dir = ''.join([data_path,f'decoder_datasets\\\\decoder_accuracy_{type_axis}_locations_{r}B_50N.npy'])\n",
    "        np.save(save_dir,locs) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108bcafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Store classifier accuracy for spatial bins in 2D\n",
    "# load the outputs from this code run_classifier_2D_3D.py\n",
    "\n",
    "n_neurons = 50\n",
    "n_var = 2 #dimensions\n",
    "\n",
    "# load predicted and test\n",
    "file = ''.join([data_path,f'decoder_datasets\\\\clf_2D_and_3D\\\\PRED_n{n_neurons}_{n_var}_SVM.npy'])\n",
    "all_predicted = np.load(file)\n",
    "file = ''.join([data_path,f'decoder_datasets\\\\clf_2D_and_3D\\\\TEST_n{n_neurons}_{n_var}_SVM.npy'])\n",
    "Y_test=np.load(file)\n",
    "\n",
    "# populate the scores of each bin\n",
    "all_scores_bins=np.zeros((all_predicted.shape[0],all_predicted.shape[1]))\n",
    "\n",
    "for bin in range(all_predicted.shape[0]): # for each bin\n",
    "    for rep in range(all_predicted.shape[1]): # for each repetition\n",
    "        delay_score = np.zeros((all_predicted.shape[2],1))\n",
    "        for k in range(all_predicted.shape[2]): # for each k fold\n",
    "            pred = np.reshape(all_predicted[bin,rep,k,:],-1)\n",
    "            test = np.reshape(Y_test[bin,rep,k,:],-1)\n",
    "\n",
    "            cm = confusion_matrix(test, pred,normalize='true')\n",
    "            score = np.diag(cm)\n",
    "            delay_score[k] = np.mean(score[:11])\n",
    "\n",
    "            if delay_score[k] ==1:\n",
    "                delay_score[k] = 0\n",
    "\n",
    "        all_scores_bins[bin,rep] = np.mean(delay_score)*100\n",
    "\n",
    "save_dir = ''.join([data_path,f'decoder_datasets\\\\decoder_accuracy_2D.npy'])\n",
    "np.save(save_dir,all_scores_bins) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ba01772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Store classifier accuracy for spatial bins in 2D\n",
    "# load the outputs from this code run_classifier_2D_3D.py\n",
    "\n",
    "n_var = 2\n",
    "n_bins = 4\n",
    "n_reps = 100\n",
    "n_neurons = 100\n",
    "# populate the scores of each bin\n",
    "all_scores=np.zeros((n_bins,n_reps,n_neurons))\n",
    "\n",
    "for bin in range(1,5):\n",
    "    # load predicted and test\n",
    "    file =''.join([data_path,f'decoder_datasets_complete\\\\clf_2D_and_3D\\\\PRED_increasingN_{n_var}_{bin}.npy'])\n",
    "    all_predicted = np.load(file)\n",
    "    file =''.join([data_path,f'decoder_datasets_complete\\\\clf_2D_and_3D\\\\TEST_increasingN_{n_var}_{bin}.npy'])\n",
    "    Y_test=np.load(file)\n",
    "    \n",
    "    for rep in range(all_predicted.shape[1]):\n",
    "        pred = all_predicted[bin-1,rep,:,:]\n",
    "        test = Y_test[bin-1,rep,:,:]\n",
    "\n",
    "        for n in range(pred.shape[0]): # for each neuron\n",
    "            \n",
    "            if n<n_neurons:\n",
    "                delay_score = np.zeros((pred.shape[1],1))\n",
    "\n",
    "                for k in range(pred.shape[1]): # for each k fold\n",
    "                    # compute the confusion matrix\n",
    "                    cm = confusion_matrix(pred[n,k,:], test[n,k,:],normalize='true')\n",
    "                    # extract the score for delays\n",
    "                    score = np.diag(cm)\n",
    "                    delay_score[k] = np.mean(score[:11])        \n",
    "\n",
    "                all_scores[bin-1,rep,n] = np.mean(delay_score)*100\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "chance_accuracy = 1/11*100\n",
    "\n",
    "all_sign = []\n",
    "for b in range(all_scores.shape[0]):\n",
    "    significant_neurons = []\n",
    "    all_pvalues = []\n",
    "    these_scores = all_scores[b,:,:]\n",
    "    for neuron_index in range(these_scores.shape[1]):  # Iterate over neurons\n",
    "        neuron_accuracy = these_scores[:, neuron_index]\n",
    "\n",
    "        # Perform Wilcoxon signed-rank test\n",
    "        _, p_value = stats.wilcoxon(neuron_accuracy - chance_accuracy, alternative='greater')  \n",
    "\n",
    "        if p_value < 0.05 and np.mean(neuron_accuracy) > chance_accuracy:\n",
    "            significant_neurons.append(neuron_index)\n",
    "            all_pvalues.append(p_value)\n",
    "    \n",
    "    #print(significant_neurons[0],all_pvalues[0])\n",
    "    #print(significant_neurons[1],all_pvalues[1])\n",
    "    \n",
    "    all_sign.append(significant_neurons[1])\n",
    "    \n",
    "save_dir = ''.join([data_path,f'decoder_datasets\\\\decoder_accuracy_2D_increasingN.npz'])\n",
    "np.savez(\n",
    "    save_dir,\n",
    "    all_scores=all_scores,\n",
    "    all_sign=np.array(all_sign, dtype=object)  # store as object array\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bacae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract control decoder output - with 25 neurons\n",
    "\n",
    "this_n = 25\n",
    "n_rep = 100\n",
    "n_dim = 24\n",
    "train_accuracy = np.zeros((n_dim,n_rep))\n",
    "test_accuracy = np.zeros((n_dim,n_rep))\n",
    "for r in range(n_rep):\n",
    "\n",
    "    all_best_C = np.load(''.join([data_path,f'decoder_datasets_complete\\\\clf_control\\\\all_best_C_delay{this_n}N_rep{r}.npy'])) # the C\n",
    "    all_train_accuracy = np.load(''.join([data_path,f'decoder_datasets_complete\\\\clf_control\\\\all_train_accuracy_delay{this_n}N_rep{r}.npy']))# the train accuracy\n",
    "    all_test_accuracy = np.load(''.join([data_path,f'decoder_datasets_complete\\\\clf_control\\\\all_test_accuracy_delay{this_n}N_rep{r}.npy']))# the test accuracy\n",
    "    \n",
    "    train_accuracy[:,r] = all_train_accuracy*100\n",
    "    test_accuracy[:,r] = all_test_accuracy*100\n",
    "\n",
    "# Define the number of neurons\n",
    "neurons = np.arange(1, n_dim + 1)\n",
    "\n",
    "save_dir = ''.join([data_path,f'decoder_datasets\\\\decoder_control_training_25N.npz'])\n",
    "np.savez(\n",
    "    save_dir,\n",
    "    train_accuracy=train_accuracy,\n",
    "    test_accuracy=test_accuracy,\n",
    "    neurons=neurons\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a420c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract control decoder output - with 25 neurons\n",
    "\n",
    "this_n = 50\n",
    "n_rep = 100\n",
    "n_dim = 24\n",
    "train_accuracy = np.zeros((n_dim,n_rep))\n",
    "test_accuracy = np.zeros((n_dim,n_rep))\n",
    "for r in range(n_rep):\n",
    "\n",
    "    all_best_C = np.load(''.join([data_path,f'decoder_datasets_complete\\\\clf_control\\\\all_best_C_delay{this_n}N_rep{r}.npy'])) # the C\n",
    "    all_train_accuracy = np.load(''.join([data_path,f'decoder_datasets_complete\\\\clf_control\\\\all_train_accuracy_delay{this_n}N_rep{r}.npy']))# the train accuracy\n",
    "    all_test_accuracy = np.load(''.join([data_path,f'decoder_datasets_complete\\\\clf_control\\\\all_test_accuracy_delay{this_n}N_rep{r}.npy']))# the test accuracy\n",
    "    \n",
    "    train_accuracy[:,r] = all_train_accuracy*100\n",
    "    test_accuracy[:,r] = all_test_accuracy*100\n",
    "\n",
    "# Define the number of neurons\n",
    "neurons = np.arange(1, n_dim + 1)\n",
    "\n",
    "save_dir = ''.join([data_path,f'decoder_datasets\\\\decoder_control_training_50N.npz'])\n",
    "np.savez(\n",
    "    save_dir,\n",
    "    train_accuracy=train_accuracy,\n",
    "    test_accuracy=test_accuracy,\n",
    "    neurons=neurons\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "67d0aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Store classifier accuracy for spatial bins in 2D\n",
    "# load the outputs from this code run_classifier_2D_3D.py\n",
    "\n",
    "n_neurons = 50\n",
    "n_var = 2 #dimensions\n",
    "\n",
    "# load predicted and test\n",
    "file = ''.join([data_path,f'decoder_datasets\\\\clf_2D_and_3D\\\\PRED_n{n_neurons}_{n_var}_SVM.npy'])\n",
    "all_predicted = np.load(file)\n",
    "file = ''.join([data_path,f'decoder_datasets\\\\clf_2D_and_3D\\\\TEST_n{n_neurons}_{n_var}_SVM.npy'])\n",
    "Y_test=np.load(file)\n",
    "\n",
    "# populate the scores of each bin\n",
    "all_scores_bins=np.zeros((all_predicted.shape[0],all_predicted.shape[1]))\n",
    "\n",
    "for bin in range(all_predicted.shape[0]): # for each bin\n",
    "    for rep in range(all_predicted.shape[1]): # for each repetition\n",
    "        delay_score = np.zeros((all_predicted.shape[2],1))\n",
    "        for k in range(all_predicted.shape[2]): # for each k fold\n",
    "            pred = np.reshape(all_predicted[bin,rep,k,:],-1)\n",
    "            test = np.reshape(Y_test[bin,rep,k,:],-1)\n",
    "\n",
    "            cm = confusion_matrix(test, pred,normalize='true')\n",
    "            score = np.diag(cm)\n",
    "            delay_score[k] = np.mean(score[:11])\n",
    "\n",
    "            if delay_score[k] ==1:\n",
    "                delay_score[k] = 0\n",
    "\n",
    "        all_scores_bins[bin,rep] = np.mean(delay_score)*100\n",
    "\n",
    "save_dir = ''.join([data_path,f'decoder_datasets\\\\decoder_accuracy_2D.npy'])\n",
    "np.save(save_dir,all_scores_bins) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c6f4b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Store classifier accuracy for spatial bins in 3D\n",
    "# load the outputs from this code run_classifier_2D_3D.py\n",
    "\n",
    "n_neurons = 50\n",
    "n_var = 3\n",
    "\n",
    "# load predicted and test\n",
    "file = ''.join([data_path,f'decoder_datasets\\\\clf_2D_and_3D\\\\PRED_n{n_neurons}_{n_var}_SVM.npy'])\n",
    "all_predicted = np.load(file)\n",
    "file = ''.join([data_path,f'decoder_datasets\\\\clf_2D_and_3D\\\\TEST_n{n_neurons}_{n_var}_SVM.npy'])\n",
    "Y_test=np.load(file)\n",
    "\n",
    "# populate the scores of each bin\n",
    "all_scores_bins=np.zeros((all_predicted.shape[0],all_predicted.shape[1]))\n",
    "\n",
    "for bin in range(all_predicted.shape[0]): # for each bin\n",
    "    for rep in range(all_predicted.shape[1]): # for each repetition\n",
    "        delay_score = np.zeros((all_predicted.shape[2],1))\n",
    "        for k in range(all_predicted.shape[2]): # for each k fold\n",
    "            pred = np.reshape(all_predicted[bin,rep,k,:],-1)\n",
    "            test = np.reshape(Y_test[bin,rep,k,:],-1)\n",
    "\n",
    "            cm = confusion_matrix(test, pred,normalize='true')\n",
    "            score = np.diag(cm)\n",
    "            delay_score[k] = np.mean(score[:11])\n",
    "\n",
    "            if delay_score[k] ==1:\n",
    "                delay_score[k] = 0\n",
    "\n",
    "        all_scores_bins[bin,rep] = np.mean(delay_score)*100\n",
    "\n",
    "scores = np.mean(all_scores_bins,axis=1)\n",
    "save_dir = ''.join([data_path,f'decoder_datasets\\\\decoder_accuracy_3D.npy'])\n",
    "np.save(save_dir,scores) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1d0f0f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract decoding accuracy along ML and AP axes for visual, auditory, multisensoryND and delay neurons \n",
    "# load the outputs from this code run_classifier_1D_modalities.py\n",
    "\n",
    "axes = ['ML','AP']\n",
    "classes = ['visual','auditory','multi no delay','delay']\n",
    "which_mod = [1,2,3,4]\n",
    "\n",
    "n = 25 # choose number of neurons\n",
    "r = 650 # bin size chosen\n",
    "count = 0\n",
    "\n",
    "for type_axis in axes:\n",
    "    \n",
    "    for l,mod in enumerate(which_mod):    \n",
    "\n",
    "        # load the predicted and the test\n",
    "        file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_PRED_n{n}_step{r}_mod{mod}.0_SVM.npy'])\n",
    "        all_predicted = np.load(file)\n",
    "        file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_TEST_n{n}_step{r}_mod{mod}.0_SVM.npy'])\n",
    "        Y_test=np.load(file)\n",
    "\n",
    "        # load bin specs\n",
    "        file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_IDS_n{n}_step{r}_mod{mod}.0_SVM.npy']) # the ids of the neurons\n",
    "        specs_neurons = np.load(file)\n",
    "        file = ''.join([data_path,f'decoder_datasets\\\\clf_1D\\\\{type_axis}_clf_BINS_n{n}_step{r}_mod{mod}.0_SVM.npy']) # the ids of the neurons # the number of session and animals in the bin \n",
    "        specs_bins = np.load(file)\n",
    "        loc = specs_bins[:,0]\n",
    "\n",
    "        for i in range(1,len(loc)):\n",
    "            loc[i]= loc[i-1]+loc[i]\n",
    "        locs = np.zeros((2,len(loc)))\n",
    "        for i in range(len(loc)):\n",
    "            if i==0:\n",
    "                locs[:,i] = [0,loc[i]]\n",
    "            else:\n",
    "                locs[:,i] = [loc[i-1],loc[i]]\n",
    "\n",
    "        scores = np.zeros((all_predicted.shape[1],all_predicted.shape[0]))\n",
    "\n",
    "        for n_bin in range(all_predicted.shape[0]): # for each bin\n",
    "            pred = all_predicted[n_bin,:,:,:]\n",
    "            test = Y_test[n_bin,:,:,:]\n",
    "\n",
    "            for rep in range(all_predicted.shape[1]): #for each repetition\n",
    "                delay_score = np.zeros((all_predicted.shape[2],1))\n",
    "                for k in range(all_predicted.shape[2]): # for each k fold\n",
    "\n",
    "                    cm = confusion_matrix(test[rep,k,:], pred[rep,k,:],normalize='true')\n",
    "                    score = np.diag(cm)\n",
    "                    delay_score[k] = np.mean(score[:11])\n",
    "                    if delay_score[k] ==1:\n",
    "                        delay_score[k] = float(\"nan\")\n",
    "                scores[rep,n_bin] = np.mean(delay_score)*100\n",
    "\n",
    "        save_dir = ''.join([data_path,f'decoder_datasets\\\\decoder_accuracy_{type_axis}_mod{mod}.npy'])\n",
    "        np.save(save_dir,scores) \n",
    "\n",
    "        save_dir = ''.join([data_path,f'decoder_datasets\\\\decoder_accuracy_{type_axis}_locations_mod{mod}.npy'])\n",
    "        np.save(save_dir,locs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038692d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
