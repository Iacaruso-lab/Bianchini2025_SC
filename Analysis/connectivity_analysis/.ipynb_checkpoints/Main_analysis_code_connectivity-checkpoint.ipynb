{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "630c5b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define relevants paths and load functions and libraries\n",
    "\n",
    "%run Bianchini2025_SC\\\\Analysis\\\\helper_functions\\\\functions_analysis.py\n",
    "    \n",
    "data_path = 'Bianchini2025_SC\\\\Datasets\\\\' # your data path\n",
    "saving_path = 'Bianchini2025_SC\\\\Figures_output\\\\' # your saving figures path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a9df889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the CCG dataset\n",
    "\n",
    "file = ''.join([data_path,'connectivity_dataset\\\\CCG_dataset_baseline.mat'])\n",
    "\n",
    "CCG_dict = mat73.loadmat(file)\n",
    "all_ccg = CCG_dict['all_ccg']\n",
    "\n",
    "for k in all_ccg.keys():\n",
    "    globals()[k] = all_ccg[k]\n",
    "    \n",
    "connection_strength = np.copy(peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1c27efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate and plot the correlation probability for both pairs and stim pairs\n",
    "def calculate_probability(pair_distance, over_tot, sig_indices, bin_width = 50, lag_sign=None):\n",
    "\n",
    "    min_val = np.min(pair_distance)\n",
    "    max_val = np.max(pair_distance)\n",
    "    n_bins = int(np.round((max_val - min_val) / bin_width))\n",
    "\n",
    "    idA, edges = makeBins_SC(pair_distance, n_bins)\n",
    "\n",
    "    idB = idA[over_tot]\n",
    "    if lag_sign is not None:\n",
    "        sig_lag_sign = lag_sign[sig_indices]\n",
    "        pairs_connected = sig_indices[sig_lag_sign != 0]\n",
    "        idC = idA[pairs_connected]\n",
    "    else:\n",
    "        idC = idA[sig_indices]\n",
    "\n",
    "    perc = []\n",
    "    for b in range(1, n_bins + 1):\n",
    "        tot = idB[idB == b].shape[0]\n",
    "        sig = idC[idC == b].shape[0]\n",
    "        if tot == 0:\n",
    "            perc.append(float('nan'))\n",
    "        elif sig == 0:\n",
    "            perc.append(0)\n",
    "        else:\n",
    "            perc.append((sig / tot) * 100)\n",
    "\n",
    "    return edges, perc, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57c18993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess probability of correlation and connection\n",
    "\n",
    "min_FR = 10#10 #Hz\n",
    "sig_idx = sig_idx_5sd # peak a SD\n",
    "\n",
    "sig_indices = np.unique(np.concatenate([np.where((np.array(sig_idx) == 1) & (np.array(pre_modality) > 0) & (np.array(post_modality) > 0))[0],\n",
    "                                        np.where((np.array(sig_idx) == 1) & (np.array(pre_mean_FR) > min_FR) & (np.array(post_mean_FR) > min_FR))[0]]))\n",
    "\n",
    "over_tot = C = np.unique(np.concatenate([np.where((np.array(pre_modality) > 0) & (np.array(post_modality) > 0))[0],\n",
    "                                             np.where((np.array(pre_mean_FR) > min_FR) & (np.array(post_mean_FR) > min_FR))[0]]))\n",
    "\n",
    "sig_pairs = sig_indices[0::2]\n",
    "tot_pairs = over_tot [0::2]\n",
    "\n",
    "# define directionality\n",
    "lag_sign = np.zeros(peak_lag.shape[0])\n",
    "lag_sign[peak_lag>=10] = 1\n",
    "lag_sign[peak_lag<=-10] = -1\n",
    "\n",
    "sig_lag_sign = lag_sign[sig_indices]\n",
    "pairs_connected = sig_indices[sig_lag_sign!=0]\n",
    "pairs_simultanous = sig_indices[sig_lag_sign==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da176efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_count = sess_n[over_tot]\n",
    "unique_values1, counts1 = np.unique(all_count, return_counts=True)\n",
    "\n",
    "all_count2 = sess_n[pairs_connected]\n",
    "unique_values2, counts2 = np.unique(all_count2, return_counts=True)\n",
    "\n",
    "# Find the intersecting values\n",
    "intersecting_values = np.intersect1d(unique_values1, unique_values2)\n",
    "\n",
    "# Find the indices of these intersecting values in unique_values1\n",
    "indices_in_unique_values1 = np.where(np.isin(unique_values1, intersecting_values))[0]\n",
    "\n",
    "conn_prob_individual_rec = counts2/counts1[indices_in_unique_values1]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b07d50b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.827549965837265"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_count = sess_n[over_tot]\n",
    "unique_values1, counts1 = np.unique(all_count, return_counts=True)\n",
    "\n",
    "all_count2 = sess_n[pairs_simultanous]\n",
    "unique_values2, counts2 = np.unique(all_count2, return_counts=True)\n",
    "\n",
    "# Find the intersecting values\n",
    "intersecting_values = np.intersect1d(unique_values1, unique_values2)\n",
    "\n",
    "# Find the indices of these intersecting values in unique_values1\n",
    "indices_in_unique_values1 = np.where(np.isin(unique_values1, intersecting_values))[0]\n",
    "\n",
    "conn_prob_individual_rec_simultaneous = counts2/counts1[indices_in_unique_values1]*100\n",
    "np.mean(conn_prob_individual_rec_simultaneous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0140c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for figure one save and perc_pairs to see how connection percentage changes with distance\n",
    "\n",
    "bin_width = 100\n",
    "edges_pairs, perc_pairs, max_val = calculate_probability(pair_distance_depth, over_tot, sig_indices,bin_width = bin_width,lag_sign=lag_sign)\n",
    "\n",
    "# and it get saved so it is easier for the plotting ot just load it\n",
    "save_dir = ''.join([data_path,'connectivity_dataset\\\\conn_perc_distance.npy'])\n",
    "np.save(save_dir,perc_pairs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0180ba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate random connection probability \n",
    "\n",
    "file = ''.join([data_path,'connectivity_dataset\\\\CCG_dataset_baseline_shuffled.mat'])\n",
    "\n",
    "CCG_dict_shuffled = mat73.loadmat(file)\n",
    "all_ccg_shuffled = CCG_dict_shuffled['all_ccg']\n",
    "\n",
    "for k in all_ccg_shuffled.keys():\n",
    "    new_name = f\"{k}_shuffled\"\n",
    "    globals()[new_name] = all_ccg_shuffled[k]\n",
    "\n",
    "lag_sign_shuffled = np.zeros(peak_lag_shuffled.shape[0])\n",
    "lag_sign_shuffled[peak_lag_shuffled>=10] = 1\n",
    "lag_sign_shuffled[peak_lag_shuffled<=-10] = -1\n",
    "\n",
    "# assess probability of correlation and connection\n",
    "\n",
    "min_FR = 10#10 #Hz\n",
    "sig_idx = sig_idx_5sd_shuffled # peak a SD\n",
    "\n",
    "sig_indices_shuffled = np.unique(np.concatenate([np.where((np.array(sig_idx) == 1) & (np.array(pre_modality_shuffled) > 0) & (np.array(post_modality_shuffled) > 0))[0],\n",
    "                                        np.where((np.array(sig_idx) == 1) & (np.array(pre_mean_FR_shuffled) > min_FR) & (np.array(post_mean_FR_shuffled) > min_FR))[0]]))\n",
    "\n",
    "over_tot_shuffled = C = np.unique(np.concatenate([np.where((np.array(pre_modality_shuffled) > 0) & (np.array(post_modality_shuffled) > 0))[0],\n",
    "                                             np.where((np.array(pre_mean_FR_shuffled) > min_FR) & (np.array(post_mean_FR_shuffled) > min_FR))[0]]))\n",
    "\n",
    "sig_pairs_shuffled = sig_indices_shuffled[0::2]\n",
    "tot_pairs_shuffled = over_tot_shuffled [0::2]\n",
    "\n",
    "sig_lag_sign_shuffled = lag_sign_shuffled[sig_indices_shuffled]\n",
    "pairs_connected_shuffled = sig_indices_shuffled[sig_lag_sign_shuffled!=0]\n",
    "\n",
    "random_connection_probability = len(pairs_connected_shuffled)/len(over_tot_shuffled)*100\n",
    "\n",
    "# and it get saved so it is easier for the plotting ot just load it\n",
    "save_dir = ''.join([data_path,'connectivity_dataset\\\\random_connection_probability.npy'])\n",
    "np.save(save_dir,random_connection_probability) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e031f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['AP_lim', 'ML_lim', 'all_boot_aud', 'all_boot_vis', 'animal_ID', 'binSize', 'coord3D', 'depth_lim', 'experiment_ID', 'modality', 'peaks', 'pvals', 'resp', 'spikes', 'trials', 'window_spikes'])\n"
     ]
    }
   ],
   "source": [
    "# load the main dataset\n",
    "file= ''.join([data_path,'neurons_datasets\\\\delay_tuning_dataset.mat'])\n",
    "data_dict = mat73.loadmat(file)\n",
    "DAT=data_dict['merged_dataset']\n",
    "\n",
    "# check keys available\n",
    "print(DAT.keys())\n",
    "\n",
    "# extract all keys\n",
    "for k in DAT.keys():\n",
    "    globals()[k] = DAT[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad7b9939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection percentage as a function of location along ML axis \n",
    "\n",
    "actual_lengthML = ML_lim[1] - ML_lim[0]\n",
    "pair_meanML_pos2 = pair_meanML_pos - ML_lim[0]\n",
    "max_val = actual_lengthML\n",
    "min_val = 0 \n",
    "new_pair_meanML_pos = np.array([max_val - val + min_val for val in pair_meanML_pos2])\n",
    "\n",
    "all_perc_conn,all_ML=[],[]\n",
    "\n",
    "for s in np.unique(sess_n):\n",
    "    this_s = np.where(sess_n==s)[0]\n",
    "    this_s = this_s[0::2]\n",
    "\n",
    "    how_many_tot = np.intersect1d(over_tot,this_s).shape[0]\n",
    "    how_many_sig = np.intersect1d(pairs_connected,this_s).shape[0]\n",
    "    perc=((how_many_sig/how_many_tot)*100) \n",
    "    all_perc_conn.append(perc)\n",
    "    \n",
    "    ML_loc = np.nanmean(new_pair_meanML_pos[this_s])\n",
    "    all_ML.append(ML_loc)\n",
    "    \n",
    "# Save the arrays in a compressed .npz file\n",
    "save_dir = ''.join([data_path,'connectivity_dataset\\\\conn_perc_ML.npz'])\n",
    "np.savez(save_dir, all_ML=all_ML, all_perc_conn=all_perc_conn, actual_lengthML=actual_lengthML)\n",
    "\n",
    "# connection percentage as a function of location along ML axis \n",
    "\n",
    "actual_lengthML = ML_lim[1] - ML_lim[0]\n",
    "pair_meanML_pos2 = pair_meanML_pos - ML_lim[0]\n",
    "max_val = actual_lengthML\n",
    "min_val = 0 \n",
    "new_pair_meanML_pos = np.array([max_val - val + min_val for val in pair_meanML_pos2])\n",
    "\n",
    "all_perc_conn,all_ML=[],[]\n",
    "\n",
    "for s in np.unique(sess_n):\n",
    "    this_s = np.where(sess_n==s)[0]\n",
    "    this_s = this_s[0::2]\n",
    "\n",
    "    how_many_tot = np.intersect1d(over_tot,this_s).shape[0]\n",
    "    how_many_sig = np.intersect1d(pairs_simultanous,this_s).shape[0]\n",
    "    perc=((how_many_sig/how_many_tot)*100) \n",
    "    all_perc_conn.append(perc)\n",
    "    \n",
    "    ML_loc = np.nanmean(new_pair_meanML_pos[this_s])\n",
    "    all_ML.append(ML_loc)\n",
    "    \n",
    "# Save the arrays in a compressed .npz file\n",
    "save_dir = ''.join([data_path,'connectivity_dataset\\\\conn_perc_ML_zerolag.npz'])\n",
    "np.savez(save_dir, all_ML=all_ML, all_perc_conn=all_perc_conn, actual_lengthML=actual_lengthML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a053e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection percentage as a function of location along ML axis \n",
    "\n",
    "actual_lengthAP = AP_lim[1] - AP_lim[0]\n",
    "new_pair_meanAP_pos = pair_meanAP_pos - AP_lim[0]\n",
    "\n",
    "all_perc_conn,all_AP=[],[]\n",
    "\n",
    "for s in np.unique(sess_n):\n",
    "    this_s = np.where(sess_n==s)[0]\n",
    "    this_s = this_s[0::2]\n",
    "\n",
    "    how_many_tot = np.intersect1d(over_tot,this_s).shape[0]\n",
    "    how_many_sig = np.intersect1d(pairs_connected,this_s).shape[0]\n",
    "    perc=((how_many_sig/how_many_tot)*100) \n",
    "    all_perc_conn.append(perc)\n",
    "    \n",
    "    AP_loc = np.nanmean(new_pair_meanAP_pos[this_s])\n",
    "    all_AP.append(AP_loc)\n",
    "    \n",
    "# Save the arrays in a compressed .npz file\n",
    "save_dir = ''.join([data_path,'connectivity_dataset\\\\conn_perc_AP.npz'])\n",
    "np.savez(save_dir, all_AP=all_AP, all_perc_conn=all_perc_conn, actual_lengthAP=actual_lengthAP)\n",
    "\n",
    "# connection percentage as a function of location along ML axis \n",
    "\n",
    "actual_lengthAP = AP_lim[1] - AP_lim[0]\n",
    "new_pair_meanAP_pos = pair_meanAP_pos - AP_lim[0]\n",
    "\n",
    "all_perc_conn,all_AP=[],[]\n",
    "\n",
    "for s in np.unique(sess_n):\n",
    "    this_s = np.where(sess_n==s)[0]\n",
    "    this_s = this_s[0::2]\n",
    "\n",
    "    how_many_tot = np.intersect1d(over_tot,this_s).shape[0]\n",
    "    how_many_sig = np.intersect1d(pairs_simultanous,this_s).shape[0]\n",
    "    perc=((how_many_sig/how_many_tot)*100) \n",
    "    all_perc_conn.append(perc)\n",
    "    \n",
    "    AP_loc = np.nanmean(new_pair_meanAP_pos[this_s])\n",
    "    all_AP.append(AP_loc)\n",
    "    \n",
    "# Save the arrays in a compressed .npz file\n",
    "save_dir = ''.join([data_path,'connectivity_dataset\\\\conn_perc_AP_zerolag.npz'])\n",
    "np.savez(save_dir, all_AP=all_AP, all_perc_conn=all_perc_conn, actual_lengthAP=actual_lengthAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1483cada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "# save AP and ML position as well as connection proabability for LMM\n",
    "count_n = []\n",
    "all_perc = []\n",
    "all_ML =[]\n",
    "all_AP = []\n",
    "for s in np.unique(sess_n):\n",
    "    this_s = np.where(sess_n==s)[0]\n",
    "    this_s = this_s[0::2]\n",
    "\n",
    "    how_many_tot = np.intersect1d(over_tot,this_s).shape[0]\n",
    "    how_many_sig = np.intersect1d(pairs_simultanous,this_s).shape[0]\n",
    "\n",
    "    perc=((how_many_sig/how_many_tot)*100) \n",
    "    \n",
    "    ML_loc = np.nanmean(new_pair_meanML_pos[this_s])\n",
    "    AP_loc = np.nanmean(pair_meanAP_pos[this_s])\n",
    "\n",
    "    all_perc.append(perc)\n",
    "    all_ML.append(ML_loc)\n",
    "    all_AP.append(AP_loc)\n",
    "    \n",
    "    unique_pre = np.unique(pre_id[this_s])\n",
    "    unique_post = np.unique(post_id[this_s])\n",
    "\n",
    "    # Find intersection\n",
    "    intersection = np.unique(np.intersect1d(unique_pre, unique_post))\n",
    "    count_n.append(intersection.shape[0])\n",
    "    \n",
    "all_perc = np.array(all_perc)\n",
    "all_ML = np.array(all_ML)\n",
    "all_AP = np.array(all_AP)\n",
    "count_n = np.array(count_n)\n",
    "\n",
    "data = {'mean_connectivity': all_perc.flatten(),\n",
    "        'ML': all_ML.flatten(),\n",
    "        'AP': all_AP.flatten(),\n",
    "        'n_neurons' : count_n.flatten()}\n",
    "df = pd.DataFrame(data)\n",
    "# Saving DataFrame to a CSV file\n",
    "df.to_csv('connectivity_individual_recordings_zerolag.csv', index=False) \n",
    "\n",
    "print('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8ab09ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection percentage as a function of location along ML axis \n",
    "\n",
    "actual_lengthML = ML_lim[1] - ML_lim[0]\n",
    "pair_meanML_pos2 = pair_meanML_pos - ML_lim[0]\n",
    "max_val = actual_lengthML\n",
    "min_val = 0 \n",
    "new_pair_meanML_pos = np.array([max_val - val + min_val for val in pair_meanML_pos2])\n",
    "\n",
    "all_perc_conn,all_ML=[],[]\n",
    "\n",
    "for s in np.unique(sess_n):\n",
    "    this_s = np.where(sess_n==s)[0]\n",
    "    this_s = this_s[0::2]\n",
    "\n",
    "    how_many_tot = np.intersect1d(over_tot,this_s).shape[0]\n",
    "    how_many_sig = np.intersect1d(pairs_simultanous,this_s).shape[0]\n",
    "    perc=((how_many_sig/how_many_tot)*100) \n",
    "    all_perc_conn.append(perc)\n",
    "    \n",
    "    ML_loc = np.nanmean(new_pair_meanML_pos[this_s])\n",
    "    all_ML.append(ML_loc)\n",
    "    \n",
    "# Save the arrays in a compressed .npz file\n",
    "save_dir = ''.join([data_path,'connectivity_dataset\\\\conn_perc_ML_lagzero.npz'])\n",
    "np.savez(save_dir, all_ML=all_ML, all_perc_conn=all_perc_conn, actual_lengthML=actual_lengthML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9dd72679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection percentage as a function of location along ML axis \n",
    "\n",
    "actual_lengthAP = AP_lim[1] - AP_lim[0]\n",
    "new_pair_meanAP_pos = pair_meanAP_pos - AP_lim[0]\n",
    "\n",
    "all_perc_conn,all_AP=[],[]\n",
    "\n",
    "for s in np.unique(sess_n):\n",
    "    this_s = np.where(sess_n==s)[0]\n",
    "    this_s = this_s[0::2]\n",
    "\n",
    "    how_many_tot = np.intersect1d(over_tot,this_s).shape[0]\n",
    "    how_many_sig = np.intersect1d(pairs_simultanous,this_s).shape[0]\n",
    "    perc=((how_many_sig/how_many_tot)*100) \n",
    "    all_perc_conn.append(perc)\n",
    "    \n",
    "    AP_loc = np.nanmean(new_pair_meanAP_pos[this_s])\n",
    "    all_AP.append(AP_loc)\n",
    "    \n",
    "# Save the arrays in a compressed .npz file\n",
    "save_dir = ''.join([data_path,'connectivity_dataset\\\\conn_perc_AP_lagzero.npz'])\n",
    "np.savez(save_dir, all_AP=all_AP, all_perc_conn=all_perc_conn, actual_lengthAP=actual_lengthAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f56169ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save distribution of peak efficacy\n",
    "\n",
    "# and it get saved so it is easier for the plotting ot just load it\n",
    "save_dir = ''.join([data_path,'connectivity_dataset\\\\conn_strenght_dist.npy'])\n",
    "np.save(save_dir,connection_strength[sig_indices]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce92e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's find delay neurons \n",
    "\n",
    "sig_del = []\n",
    "which_tr = []\n",
    "for i in range(peaks.shape[0]):\n",
    "    y = peaks[i,:-2]\n",
    "\n",
    "    vis_FR = peaks[i,-2]\n",
    "    aud_FR = peaks[i,-1]\n",
    "\n",
    "    if vis_FR>aud_FR:\n",
    "        boot_out = all_boot_vis[i,:]\n",
    "    elif aud_FR>vis_FR:\n",
    "        boot_out = all_boot_aud[i,:]\n",
    "    \n",
    "    pos_sig = np.argwhere(boot_out>0)\n",
    "    \n",
    "    if len(pos_sig)>0:\n",
    "        sig_del.append(i)\n",
    "        tr = pos_sig[np.argmax(y[pos_sig])]\n",
    "        which_tr.append(tr) \n",
    "\n",
    "sig_del = np.array(sig_del)\n",
    "pref_delay=np.array(which_tr)[:,0]\n",
    "\n",
    "# define the four modalitties: vis, aud, delay and multisensory not delay tuned\n",
    "\n",
    "pair_boot_out = np.zeros((len(pre_id),2))\n",
    "\n",
    "for i in sig_del:\n",
    "    pair_boot_out[np.where(pre_id==i+1),0] = 1\n",
    "    pair_boot_out[np.where(post_id==i+1),1] = 1\n",
    "\n",
    "# these are the pairs that have a significant peak delay\n",
    "sig_del_pairs = np.argwhere(np.sum(pair_boot_out,axis=1)==2)\n",
    "\n",
    "# 2- get the preferred delay of these neurons\n",
    "pair_pref_delay = np.nan*np.zeros((len(pre_id),2))\n",
    "\n",
    "count = 0\n",
    "for i in sig_del:\n",
    "    pair_pref_delay[np.where(pre_id==i+1),0] = pref_delay[count]\n",
    "    pair_pref_delay[np.where(post_id==i+1),1] = pref_delay[count]\n",
    "    count+=1\n",
    "\n",
    "# for now just divide between before 50ms and after 50 ms\n",
    "sig_del_pairs_simplified = np.copy(pair_pref_delay)\n",
    "sig_del_pairs_simplified[np.where(sig_del_pairs_simplified<=5)] = 1\n",
    "sig_del_pairs_simplified[np.where(sig_del_pairs_simplified>=5)] = 1\n",
    "\n",
    "# create a matrix with the modalities of the pairs + a different modality if the pair is delay preferring\n",
    "pair_mod_delay = np.copy(pair_modality)\n",
    "pair_mod_delay[pair_mod_delay==5] = 1\n",
    "pair_mod_delay[pair_mod_delay==6] = 2\n",
    "\n",
    "pair_mod_delay[pair_mod_delay>=3] = 4\n",
    "pair_mod_delay[np.where(sig_del_pairs_simplified==1)] = 3\n",
    "pair_mod_delay[np.where(sig_del_pairs_simplified==2)] = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b540b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the connectivity and directionality matrix \n",
    "\n",
    "combinations = []\n",
    "n_modalities = 4\n",
    "\n",
    "how_many_bins = 1\n",
    "bin_width =2000\n",
    "\n",
    "\n",
    "var1,var2, var3, var4, var5, var6 = pair_distance_depth, pair_modality, pair_mod_delay, over_tot, sig_indices, lag_sign \n",
    "\n",
    "min_val = np.min(var1) #min distance\n",
    "max_val = np.max(var1) # max distance\n",
    "n_bins = int(np.round((max_val - min_val) / bin_width))\n",
    "\n",
    "idA, edges = makeBins_SC(var1, n_bins)\n",
    "# Generate all combinations of modalities including cases where both modalities are the same\n",
    "for i in range(1, n_modalities + 1):\n",
    "    for j in range(1, n_modalities + 1):\n",
    "        combinations.append([i, j])\n",
    "# Convert the list of combinations to a NumPy array\n",
    "cases = np.array(combinations)\n",
    "\n",
    "perc_conn = np.nan*np.zeros((n_modalities,n_modalities))\n",
    "real_number_conn = np.nan*np.zeros((n_modalities,n_modalities,how_many_bins))\n",
    "tot_number_conn = np.nan*np.zeros((n_modalities,n_modalities,how_many_bins))\n",
    "dir_score = np.nan*np.zeros((n_modalities,n_modalities))\n",
    "conn_st_mean = np.nan*np.zeros((n_modalities,n_modalities,how_many_bins))\n",
    "conn_st_median = np.nan*np.zeros((n_modalities,n_modalities,how_many_bins))\n",
    "\n",
    "for i in range(cases.shape[0]):\n",
    "\n",
    "    n_to_n = np.where((var3[:,0] == cases[i,0]) & (var3[:,1] == cases[i,1]))[0]\n",
    "\n",
    "    if cases[i,0] == cases[i,1]: # if it's reciprocal, keep only one\n",
    "        n_to_n = n_to_n[0::2]\n",
    "    \n",
    "    these_ids = np.intersect1d(var4,n_to_n) #these are the pairs that are in this modality\n",
    "    sig_these_ids = np.intersect1d(var5,these_ids) #these are the pairs that are significant in this modality\n",
    "\n",
    "    # what are the lags in these pairs?\n",
    "    these_lags = var6[sig_these_ids]\n",
    "\n",
    "    for b in range(how_many_bins):\n",
    "\n",
    "        tot_bin = np.argwhere((idA[these_ids] == b+1)) #which connections are in this bin?\n",
    "        these_mod_bin = np.argwhere((idA[sig_these_ids] == b+1))\n",
    "        these_lag_bin = these_lags[idA[sig_these_ids] == b+1]\n",
    "\n",
    "        if cases[i,0] == cases[i,1]: # if it's reciprocal\n",
    "            this_lags = np.argwhere(these_lag_bin !=0)\n",
    "            neg_lag = np.argwhere(these_lag_bin !=0)\n",
    "        else:\n",
    "            this_lags = np.argwhere(these_lag_bin <0)\n",
    "            neg_lag = np.argwhere(these_lag_bin >0)\n",
    "\n",
    "        if tot_bin.shape[0] != 0:          \n",
    "            perc_conn[cases[i,0]-1,cases[i,1]-1] = this_lags.shape[0]/tot_bin.shape[0]*100\n",
    "\n",
    "        real_number_conn[cases[i,0]-1,cases[i,1]-1,b] = this_lags.shape[0]    \n",
    "        tot_number_conn[cases[i,0]-1,cases[i,1]-1,b] = tot_bin.shape[0]\n",
    "        \n",
    "        conn_st_median[cases[i,0]-1,cases[i,1]-1,b] = np.nanmedian(connection_strength[sig_these_ids])\n",
    "        conn_st_mean[cases[i,0]-1,cases[i,1]-1,b] = np.nanmean(connection_strength[sig_these_ids])\n",
    "        \n",
    "        if cases[i,0] < cases[i,1]:\n",
    "            if this_lags.shape[0] + neg_lag.shape[0] == 0:\n",
    "                dir_score[cases[i,0]-1,cases[i,1]-1] = math.nan\n",
    "            else:\n",
    "                dir_score[cases[i,0]-1,cases[i,1]-1] = (this_lags.shape[0] - neg_lag.shape[0]) / (this_lags.shape[0] + neg_lag.shape[0])\n",
    "                \n",
    "# and it get saved so it is easier for the plotting ot just load it\n",
    "save_dir = ''.join([data_path,'connectivity_dataset\\\\connectivity_matrix.npy'])\n",
    "np.save(save_dir,perc_conn)\n",
    "\n",
    "# and it get saved so it is easier for the plotting ot just load it\n",
    "save_dir = ''.join([data_path,'connectivity_dataset\\\\directionality_score_matrix.npy'])\n",
    "np.save(save_dir,dir_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "946a55e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_bins = 1\n",
    "# all cases wanted \n",
    "cases_arrays = [ \n",
    "                np.array([[1,1],[2,1],[3,1],[4,1]]), # to visual  \n",
    "                np.array([[1,2],[2,2],[3,2],[4,2]]), # to auditory] \n",
    "                np.array([[1,3],[2,3],[3,3],[4,3]]), # to delay\n",
    "                np.array([[1,4],[2,4],[3,4],[4,4]])]# to multi no delay\n",
    "                \n",
    "                \n",
    "cases_arrays_inputs = np.copy(cases_arrays)\n",
    "\n",
    "# Initialize an empty list to store the results for each case\n",
    "conn_results = np.zeros((len(cases_arrays),len(cases_arrays[0])))\n",
    "conn_strength = np.zeros((len(cases_arrays),len(cases_arrays[0])))\n",
    "count = 0\n",
    "for cases in cases_arrays_inputs:\n",
    "\n",
    "    for i in range(cases.shape[0]):\n",
    "        \n",
    "        n_to_n = np.where((pair_mod_delay[:,0] == cases[i,0]) & (pair_mod_delay[:,1] == cases[i,1]))[0]\n",
    "\n",
    "        if cases[i,0] == cases[i,1]: # if it's reciprocal, keep only one\n",
    "            n_to_n = n_to_n[0::2]\n",
    "        \n",
    "        these_ids = np.intersect1d(over_tot,n_to_n) #these are the pairs that are in this modality\n",
    "        sig_these_ids = np.intersect1d(sig_indices,these_ids) #these are the pairs that are significant in this modality\n",
    "        \n",
    "        # what are the lags in these pairs?\n",
    "        these_lags = lag_sign[sig_these_ids]\n",
    "        \n",
    "        for b in range(how_many_bins):\n",
    "\n",
    "            tot_bin = np.argwhere((idA[these_ids] == b+1)) #which connections are in this bin?\n",
    "            these_mod_bin = np.argwhere((idA[sig_these_ids] == b+1))\n",
    "            these_lag_bin = these_lags[idA[sig_these_ids] == b+1]\n",
    "\n",
    "            if cases[i,0] == cases[i,1]: # if it's reciprocal\n",
    "                this_lags = np.argwhere(these_lag_bin !=0)\n",
    "            else:\n",
    "                this_lags = np.argwhere(these_lag_bin <0)\n",
    "\n",
    "\n",
    "            conn_results[count,i] = this_lags.shape[0]\n",
    "            good_pos = sig_these_ids[this_lags]\n",
    "            conn_strength[count,i] = np.nansum(connection_strength[good_pos])\n",
    "\n",
    "    count += 1\n",
    "\n",
    "# now let's make this into percentages\n",
    "results_strength_input = [] # median connection strength\n",
    "for i in range(conn_results.shape[1]):\n",
    "\n",
    "    results_strength_input.append(conn_strength[:,i]/np.nansum(conn_strength,axis=1)*100)\n",
    "\n",
    "results_strength_input = np.array(results_strength_input)\n",
    "results_strength_input[np.isnan(results_strength_input)] = 0\n",
    "\n",
    "# same for the outputs\n",
    "\n",
    "cases_arrays_outputs =  [np.fliplr(array) for array in cases_arrays]\n",
    "\n",
    "# Initialize an empty list to store the results for each case\n",
    "conn_results = np.zeros((len(cases_arrays),len(cases_arrays[0])))\n",
    "conn_strength = np.zeros((len(cases_arrays),len(cases_arrays[0])))\n",
    "count = 0\n",
    "for cases in cases_arrays_outputs:\n",
    "\n",
    "    for i in range(cases.shape[0]):\n",
    "        \n",
    "        n_to_n = np.where((pair_mod_delay[:,0] == cases[i,0]) & (pair_mod_delay[:,1] == cases[i,1]))[0]\n",
    "\n",
    "        if cases[i,0] == cases[i,1]: # if it's reciprocal, keep only one\n",
    "            n_to_n = n_to_n[0::2]\n",
    "        \n",
    "        these_ids = np.intersect1d(over_tot,n_to_n) #these are the pairs that are in this modality\n",
    "        sig_these_ids = np.intersect1d(sig_indices,these_ids) #these are the pairs that are significant in this modality\n",
    "        \n",
    "        # what are the lags in these pairs?\n",
    "        these_lags = lag_sign[sig_these_ids]\n",
    "        \n",
    "        for b in range(how_many_bins):\n",
    "\n",
    "            tot_bin = np.argwhere((idA[these_ids] == b+1)) #which connections are in this bin?\n",
    "            these_mod_bin = np.argwhere((idA[sig_these_ids] == b+1))\n",
    "            these_lag_bin = these_lags[idA[sig_these_ids] == b+1]\n",
    "\n",
    "            if cases[i,0] == cases[i,1]: # if it's reciprocal\n",
    "                this_lags = np.argwhere(these_lag_bin !=0)\n",
    "            else:\n",
    "                this_lags = np.argwhere(these_lag_bin <0)\n",
    "\n",
    "\n",
    "            conn_results[count,i] = this_lags.shape[0]\n",
    "            good_pos = sig_these_ids[this_lags]\n",
    "            conn_strength[count,i] = np.nansum(connection_strength[good_pos])\n",
    "\n",
    "    count += 1\n",
    "\n",
    "# now let's make this into percentages\n",
    "results_strength_output = [] # median connection strength\n",
    "for i in range(conn_results.shape[1]):\n",
    "\n",
    "    results_strength_output.append(conn_strength[:,i]/np.nansum(conn_strength,axis=1)*100)\n",
    "\n",
    "results_strength_output = np.array(results_strength_output)\n",
    "results_strength_output[np.isnan(results_strength_output)] = 0\n",
    "\n",
    "# Save the arrays in a compressed .npz file\n",
    "save_dir = ''.join([data_path,'connectivity_dataset\\\\conn_strength_IN_OUT.npz'])\n",
    "np.savez(save_dir, results_strength_input=results_strength_input, results_strength_output=results_strength_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7689ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean locations for each neuron type \n",
    "\n",
    "depth_norm2 =  ((coord3D[:,1] -depth_lim[0]) / (depth_lim[1] - depth_lim[0]))\n",
    "depth = []\n",
    "for i in range(1,5):\n",
    "    mean_depth = np.nanmean(depth_norm2[pre_id[np.argwhere(pair_mod_delay[:,0]==i)].astype(int)])    \n",
    "    depth.append(mean_depth)\n",
    "\n",
    "# and it get saved so it is easier for the plotting ot just load it\n",
    "save_dir = ''.join([data_path,'connectivity_dataset\\\\mean_loc_subgroups.npy'])\n",
    "np.save(save_dir,depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8bc7f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save mean FR during baseline for the same neurons\n",
    "\n",
    "n_modalities = 4\n",
    "\n",
    "all_data = []\n",
    "all_labels = []\n",
    "medians = []\n",
    "data_by_modality = []\n",
    "iqrs=[]\n",
    "for i in range(1, n_modalities+1):\n",
    "    FR_baseline = np.unique(pre_mean_FR[pair_mod_delay[:, 0] == i])\n",
    "    all_data.extend(FR_baseline)\n",
    "    all_labels.extend([i] * len(FR_baseline))\n",
    "    medians.append(np.median(FR_baseline))\n",
    "    q75, q25 = np.percentile(FR_baseline, [75 ,25])\n",
    "    iqrs.append(q75 - q25)\n",
    "    data_by_modality.append(FR_baseline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d2b7a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's calculate an overall connection probability\n",
    "var1,var2, var3, var4, var5, var6 = pair_distance_depth, pair_modality, pair_mod_delay, over_tot, sig_indices, lag_sign \n",
    "\n",
    "n_modalities = 4\n",
    "perc_conn = np.zeros((n_modalities,1))\n",
    "# Generate all combinations of modalities including cases where both modalities are the same\n",
    "for i in range(1, n_modalities + 1):\n",
    "\n",
    "    n_to_n = np.where((var3[:,0] == i))[0]\n",
    "    mutual_cases = np.argwhere(var3[n_to_n][:,1]==i)[0::2]\n",
    "    n_to_n_final = np.setdiff1d(n_to_n,mutual_cases)\n",
    "    \n",
    "    these_ids = np.intersect1d(var4,n_to_n_final) #these are the pairs that are in this modality\n",
    "    sig_these_ids = np.intersect1d(var5,these_ids) #these are the pairs that are significant in this modality\n",
    "    \n",
    "    # what are the lags in these pairs?\n",
    "    these_lags = var6[sig_these_ids]\n",
    "    conn_n = np.argwhere(these_lags!=0).shape[0]\n",
    "    tot_conn = these_ids.shape[0]\n",
    "    perc_conn[i-1] = (conn_n/tot_conn)*100\n",
    "    \n",
    "    \n",
    "# and it get saved so it is easier for the plotting ot just load it\n",
    "save_dir = ''.join([data_path,'connectivity_dataset\\\\FR_subgroups.npz'])\n",
    "np.savez(save_dir, all_labels=all_labels, all_data=all_data, medians=medians,perc_conn=perc_conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "50fda3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in connection percentage between medial and lateral SC\n",
    "\n",
    "# Define key variables\n",
    "var1, var3, var4, var5, var6 = pair_distance_depth, pair_mod_delay, over_tot, sig_indices, lag_sign  \n",
    "n_modalities = 4\n",
    "\n",
    "# Normalize coordinates  \n",
    "good_pos = ~np.isnan(coord3D[:, 0])  # Filter valid positions\n",
    "AP_norm = coord3D[good_pos, 0] - AP_lim[0]\n",
    "ML_norm = actual_lengthML - (coord3D[good_pos, 2] - ML_lim[0])  # Flip ML coordinates\n",
    "experiment_IDs = experiment_ID[good_pos]\n",
    "\n",
    "# Create bins\n",
    "actual_lengthML = ML_lim[1] - ML_lim[0]\n",
    "actual_lengthAP = AP_lim[1] - AP_lim[0]\n",
    "id_AP, edges_AP = makeBins_SC(np.append(AP_norm, [0, actual_lengthAP]), 2)\n",
    "id_ML, edges_ML = makeBins_SC(np.append(ML_norm, [0, actual_lengthML]), 2)\n",
    "id_AP, id_ML = id_AP[:-2], id_ML[:-2]  \n",
    "\n",
    "# Distance binning\n",
    "n_bins = int(np.round((np.max(var1) - np.min(var1)) / 2000))  \n",
    "idA, edges = makeBins_SC(var1, n_bins)\n",
    "\n",
    "# Compute connection percentages\n",
    "perc_conn = np.full((2, n_modalities, n_modalities, 1), np.nan)\n",
    "count = 0\n",
    "\n",
    "for bin in np.unique(id_ML):  \n",
    "    sessions = np.unique(experiment_IDs[id_ML == bin])\n",
    "\n",
    "    # Generate modality combinations\n",
    "    cases = np.array([[i, j] for i in range(1, n_modalities + 1) for j in range(1, n_modalities + 1)])\n",
    "\n",
    "    for i in range(cases.shape[0]):\n",
    "        n_to_n = np.where((var3[:, 0] == cases[i, 0]) & (var3[:, 1] == cases[i, 1]))[0]\n",
    "        if cases[i, 0] == cases[i, 1]:  \n",
    "            n_to_n = n_to_n[0::2]  \n",
    "\n",
    "        # Filter connections by session\n",
    "        session_mask = np.isin(sess_n, sessions)\n",
    "        n_to_n = n_to_n[session_mask[n_to_n]]\n",
    "        over_tot_filtered = var4[session_mask[var4]]\n",
    "\n",
    "        these_ids = np.intersect1d(over_tot_filtered, n_to_n)\n",
    "        sig_these_ids = np.intersect1d(var5, these_ids)\n",
    "\n",
    "        these_lags = var6[sig_these_ids]\n",
    "\n",
    "        for b in range(1):  # Single bin case\n",
    "            tot_bin = np.argwhere(idA[these_ids] == b + 1)\n",
    "            these_lag_bin = these_lags[idA[sig_these_ids] == b + 1]\n",
    "\n",
    "            if cases[i, 0] == cases[i, 1]:  \n",
    "                this_lags = np.argwhere(these_lag_bin == 0)\n",
    "            else:\n",
    "                this_lags = np.argwhere(these_lag_bin == 0)\n",
    "\n",
    "            if tot_bin.shape[0] != 0:\n",
    "                perc_conn[count, cases[i, 0] - 1, cases[i, 1] - 1, b] = this_lags.shape[0] / tot_bin.shape[0] * 100\n",
    "\n",
    "    count += 1\n",
    "    \n",
    "# and it get saved so it is easier for the plotting ot just load it\n",
    "save_dir = ''.join([data_path,'connectivity_dataset\\\\connectivity_matrix_MminumsL_zeroCCG.npy'])\n",
    "np.save(save_dir,perc_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "120815dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in connection percentage between medial and lateral SC\n",
    "\n",
    "# Define key variables\n",
    "var1, var3, var4, var5, var6 = pair_distance_depth, pair_mod_delay, over_tot, sig_indices, lag_sign  \n",
    "n_modalities = 4\n",
    "\n",
    "# Normalize coordinates  \n",
    "good_pos = ~np.isnan(coord3D[:, 0])  # Filter valid positions\n",
    "AP_norm = coord3D[good_pos, 0] - AP_lim[0]\n",
    "ML_norm = actual_lengthML - (coord3D[good_pos, 2] - ML_lim[0])  # Flip ML coordinates\n",
    "experiment_IDs = experiment_ID[good_pos]\n",
    "\n",
    "# Create bins\n",
    "id_AP, edges_AP = makeBins_SC(np.append(AP_norm, [0, actual_lengthAP]), 2)\n",
    "id_ML, edges_ML = makeBins_SC(np.append(ML_norm, [0, actual_lengthML]), 2)\n",
    "id_AP, id_ML = id_AP[:-2], id_ML[:-2]  \n",
    "\n",
    "# Distance binning\n",
    "n_bins = int(np.round((np.max(var1) - np.min(var1)) / 2000))  \n",
    "idA, edges = makeBins_SC(var1, n_bins)\n",
    "\n",
    "# Compute connection percentages\n",
    "perc_conn = np.full((2, n_modalities, n_modalities, 1), np.nan)\n",
    "count = 0\n",
    "\n",
    "for bin in np.unique(id_ML):  \n",
    "    sessions = np.unique(experiment_IDs[id_ML == bin])\n",
    "\n",
    "    # Generate modality combinations\n",
    "    cases = np.array([[i, j] for i in range(1, n_modalities + 1) for j in range(1, n_modalities + 1)])\n",
    "\n",
    "    for i in range(cases.shape[0]):\n",
    "        n_to_n = np.where((var3[:, 0] == cases[i, 0]) & (var3[:, 1] == cases[i, 1]))[0]\n",
    "        if cases[i, 0] == cases[i, 1]:  \n",
    "            n_to_n = n_to_n[0::2]  \n",
    "\n",
    "        # Filter connections by session\n",
    "        session_mask = np.isin(sess_n, sessions)\n",
    "        n_to_n = n_to_n[session_mask[n_to_n]]\n",
    "        over_tot_filtered = var4[session_mask[var4]]\n",
    "\n",
    "        these_ids = np.intersect1d(over_tot_filtered, n_to_n)\n",
    "        sig_these_ids = np.intersect1d(var5, these_ids)\n",
    "\n",
    "        these_lags = var6[sig_these_ids]\n",
    "\n",
    "        for b in range(1):  # Single bin case\n",
    "            tot_bin = np.argwhere(idA[these_ids] == b + 1)\n",
    "            these_lag_bin = these_lags[idA[sig_these_ids] == b + 1]\n",
    "\n",
    "            if cases[i, 0] == cases[i, 1]:  \n",
    "                this_lags = np.argwhere(these_lag_bin != 0)\n",
    "            else:\n",
    "                this_lags = np.argwhere(these_lag_bin < 0)\n",
    "\n",
    "            if tot_bin.shape[0] != 0:\n",
    "                perc_conn[count, cases[i, 0] - 1, cases[i, 1] - 1, b] = this_lags.shape[0] / tot_bin.shape[0] * 100\n",
    "\n",
    "    count += 1\n",
    "    \n",
    "# and it get saved so it is easier for the plotting ot just load it\n",
    "save_dir = ''.join([data_path,'connectivity_dataset\\\\connectivity_matrix_MminumsL.npy'])\n",
    "np.save(save_dir,perc_conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4dd4da29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which delay prefering neurons are connected to which?  - save the confusion matrix\n",
    "\n",
    "n_modalities = 4\n",
    "# Initialize an empty list to store the combinations\n",
    "combinations = []\n",
    "# Generate all combinations of modalities including cases where both modalities are the same\n",
    "for i in range(1, n_modalities + 1):\n",
    "    for j in range(1, n_modalities + 1):\n",
    "        combinations.append([i, j])\n",
    "# Convert the list of combinations to a NumPy array\n",
    "cases =np.array(combinations)\n",
    "\n",
    "all_pre = []\n",
    "all_post = []\n",
    "count = 0\n",
    "for i in range(cases.shape[0]):\n",
    "    \n",
    "    if cases[i,0] == 3 and cases[i,1] ==3: # if it's a delay pair  \n",
    "        n_to_n = np.where((pair_mod_delay[:,0] == cases[i,0]) & (pair_mod_delay[:,1] == cases[i,1]))[0]\n",
    "\n",
    "        these_ids = np.intersect1d(over_tot,n_to_n) #these are the pairs that are in this modality\n",
    "        sig_these_ids = np.intersect1d(sig_indices,these_ids) #these are the pairs that are significant in this modality\n",
    "\n",
    "        # what are the lags in these pairs?\n",
    "        these_lags = lag_sign[sig_these_ids]\n",
    "        \n",
    "        for b in range(how_many_bins):\n",
    "            \n",
    "            tot_bin = np.argwhere((idA[these_ids] == b+1)) #which connections are in this bin?\n",
    "            these_mod_bin = np.argwhere((idA[sig_these_ids] == b+1))\n",
    "            these_lag_bin = these_lags[idA[sig_these_ids] == b+1]\n",
    "            \n",
    "            lags_loc = sig_these_ids[np.argwhere(these_lag_bin <0)]\n",
    "\n",
    "            #check the preferred delay at this location\n",
    "            pair_pref_delay # this is the preferred delay for every neurons pair\n",
    "            pref_del_pre = pair_pref_delay[lags_loc,0]\n",
    "            pref_del_post = pair_pref_delay[lags_loc,1]\n",
    "\n",
    "            all_pre.append(pref_del_pre)\n",
    "            all_post.append(pref_del_post)\n",
    "\n",
    "# and it get saved so it is easier for the plotting ot just load it\n",
    "save_dir = ''.join([data_path,'connectivity_dataset\\\\preferred_delay_connectivity.npz'])\n",
    "np.savez(save_dir, all_pre=all_pre, all_post=all_post)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
